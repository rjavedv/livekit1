{"docstore/metadata": {"e0d6e422-0a18-426e-a265-df534c16f15a": {"doc_hash": "d990bdcd5c817f7038253efb24d83c7081441e78bee8210c520d89f33eef8da2"}, "a95a2aeb-0ecf-4696-a427-3e341b3df175": {"doc_hash": "2144068f01ce4ffb6eb03179888cfbd68521eb802370b5afaf3c1700accad21a"}, "dda03ec5-6147-4ed3-88b8-98cb2a2871aa": {"doc_hash": "41a4cbd58e92e40a635b9dccc36e23f3044f36246db94eaaae511afd41cb40e0"}, "318c1074-7f40-42ba-a51e-22e18e141ea4": {"doc_hash": "cc10784d928a8d948418aba585db10065636ca59244ba4d791f6654923829467"}, "3a975e2b-6fd7-4f9b-9ef5-1c5f0557fde5": {"doc_hash": "1dd86cf31931a7d92875a00adc6b39f28c4dfbbec8a5b60ce1f4a186baead904"}, "2e78b880-2bf8-4a86-87e0-d102570ee179": {"doc_hash": "3f6e12b3d60d2a14a8f018c163be5f38e331da654956b6fa3a89f0f50a3f5b6f"}, "41e9d02f-7d9e-4dd2-85f7-8cd3cf48e2fc": {"doc_hash": "2d76ade7477d4929d187e1e398165bfadf958114e5a357dea1038194325f7c3e"}, "ce758829-0c38-406c-9d34-99cf3b9cfb7a": {"doc_hash": "9cac35e5d04ccc3e4a798e2b5da7c40ed3f71794f8fbb54ba234a4a57130ef57"}, "4bc12972-10b0-4afb-82d6-61ffa14b87b1": {"doc_hash": "b06e04bc65f080c1caf8e830bf95329d1e471798d0ac37fef0fdfe7f8200b6f8"}, "aeec18ab-a4cb-4416-8d57-7e4fb4f506f1": {"doc_hash": "0fdfb6998e5c1e39564416879cfb880d085f8159271dd350e6742403bd783e02"}, "45ca1f90-8479-4a36-b0e4-fd415af66b67": {"doc_hash": "fef343020b0f8f8b7b893491c60453a86290442a5a4a3c6b0f551fa4c99f2baa"}, "09a90510-e7ba-4560-9f63-74eada0d2133": {"doc_hash": "124394932d1ccae05b0a1526f5ada3a7fdefc6690ce274b5d384bf6a6c896655"}, "338c2406-61e6-40c5-b0c7-f2d02ce879c8": {"doc_hash": "64a0840f6f127a64bd5cd2c8e19813025356618e47f713eaeef9fd29ffeecdf6"}, "424c989d-da09-43a6-baad-dce8655faafc": {"doc_hash": "facc4a82c781fcccfd237189be9a183494e1f8b4f6da5573d3315564fd452c4b"}, "34b6567e-5b51-47e4-a4ce-89ad91402199": {"doc_hash": "6aba90a5bb61712d983f4cebf0bab2b7a9df1896fe3de6506fe7e1189b77d13e"}, "628e0357-e7d6-456e-af9a-e48edd798271": {"doc_hash": "42fd8534e9669b37cc085267fdbb83d03d02740eb4b52c151a4b05b68036d006"}, "d2f7fd52-7267-4f8d-9718-1c265fc8a861": {"doc_hash": "bbf16405aeb4c03e020f84c15575de3ab187b86bb783ead9ee8e51ab672dd94b"}, "c32666e6-17d9-4c09-a669-3580c34f3b3e": {"doc_hash": "9914251a1f89be4e3edbe17c06f9235e713d1695bec6aaddf6bdbab62d71545f"}, "20d250c1-698e-4033-9aec-8a5c398eb4e4": {"doc_hash": "c7ead0fad39f04bd98086c6070b3da00154495b0a8621d14252ee3a4b7abee84"}, "517d8891-51d9-400b-bcb9-77e275ee3c41": {"doc_hash": "aeb0581f32ca1a56daed4fa836196841a57fd18b22bb371c934b36ee0f07832f"}, "c30296b3-e35b-4f39-b86f-8bb99f1db9d5": {"doc_hash": "f74a3ce5bf813d00bddc42b418649ded8009ed3f9dc4edc363caac93b874767e"}, "d3c7215b-66ac-4da7-b6eb-c74ef1f8d72b": {"doc_hash": "44db2523cd920bcf03a602134025648c6868bd16cfcdd7652284f63995e9d85a", "ref_doc_id": "e0d6e422-0a18-426e-a265-df534c16f15a"}, "82149970-b769-4e85-ba76-5c82042b38c7": {"doc_hash": "86cf5207528de4189a690ccb34612e76973dd4ca80313ee057d8dacbd7f9769b", "ref_doc_id": "a95a2aeb-0ecf-4696-a427-3e341b3df175"}, "41646838-e44b-4f92-b3f9-2352aee3b270": {"doc_hash": "4d071424165d68771e3ccb8ba98ad58677c0706fdbf85d41ae2a2e78d6938fa4", "ref_doc_id": "a95a2aeb-0ecf-4696-a427-3e341b3df175"}, "cf667c32-3f4f-4795-a7ee-f86577e42b6e": {"doc_hash": "7d0410d5d7b1108d80c1bfb17a7ed172213ed47e2466768e63d93d0c63d7ae8c", "ref_doc_id": "dda03ec5-6147-4ed3-88b8-98cb2a2871aa"}, "a8605c42-dee2-4ded-9ffd-a02c49a95f09": {"doc_hash": "3064be691c35c7ceac097d3dcb169c78bb279740c018921f2a8fd5847c0b31bc", "ref_doc_id": "dda03ec5-6147-4ed3-88b8-98cb2a2871aa"}, "bf575bae-5bde-455a-bc2d-98af1b61d0f8": {"doc_hash": "1345170dcdc409038327da720f15d5f9cb460934000c51e8af80477731d01928", "ref_doc_id": "318c1074-7f40-42ba-a51e-22e18e141ea4"}, "90e95fad-8171-4790-8503-14359a2cf289": {"doc_hash": "f3dec50daef175f65991a21c5b947f78b5a227b4a76242f876d6ddfd76067178", "ref_doc_id": "3a975e2b-6fd7-4f9b-9ef5-1c5f0557fde5"}, "f262115b-e579-4fe6-a3f3-b872c05747b6": {"doc_hash": "19933a7163f06cbadd3cd8c5630b0ef25892f2c892ea4b3c7dfccca77942cf4a", "ref_doc_id": "3a975e2b-6fd7-4f9b-9ef5-1c5f0557fde5"}, "e2a8683b-3c7b-4a67-af60-996befea0c43": {"doc_hash": "e037a06c35ac3b6c09731401bdab29e0253b09acd892d118bb3a183bf572f44f", "ref_doc_id": "2e78b880-2bf8-4a86-87e0-d102570ee179"}, "775dae96-69c0-4a03-ab6e-5d490ea914ea": {"doc_hash": "62406e8ba9d5948749741c1e4f9e867494dfad34ea2079136eaba1116620e9ca", "ref_doc_id": "2e78b880-2bf8-4a86-87e0-d102570ee179"}, "e51191d0-7aaf-4d55-a687-292c456ac936": {"doc_hash": "4fdec2a5002b131d92ec346b5fe4e4690da2dc4b8d075ba839492204b7536b54", "ref_doc_id": "41e9d02f-7d9e-4dd2-85f7-8cd3cf48e2fc"}, "a82810d6-3525-4e78-9488-075619653cb5": {"doc_hash": "65f35c9dfc5e427797be1c3e3cf2111ecfbcdb4a66890e0d60f7e9970ab6e097", "ref_doc_id": "ce758829-0c38-406c-9d34-99cf3b9cfb7a"}, "6e64cfe7-17fa-4099-9eca-1706ab9a40cf": {"doc_hash": "140a29d52995d43029562e8801579ff4cd4e2d195254f6ba3d3a9e2ce7dd3b52", "ref_doc_id": "ce758829-0c38-406c-9d34-99cf3b9cfb7a"}, "c2bed3b4-3c8c-407c-a4d6-2183baba9fee": {"doc_hash": "87239282ab435fedfb70fc0ad9482eb9ed28d8606362aeced818c497e87dcc88", "ref_doc_id": "4bc12972-10b0-4afb-82d6-61ffa14b87b1"}, "0e9fc9c4-6a8f-4ad9-b09d-fd1f6742f464": {"doc_hash": "44a2ed4bd36c2c7093cb6b1f0f651fc51f768174f3ff14f44d2545ddcd9e22fa", "ref_doc_id": "4bc12972-10b0-4afb-82d6-61ffa14b87b1"}, "5aa2686f-712f-48d0-95fa-a9ccac8876f1": {"doc_hash": "77dc934b6005b44f742c61249745499f21a64ada5882263d578818ff758cdc6c", "ref_doc_id": "4bc12972-10b0-4afb-82d6-61ffa14b87b1"}, "4d9ad5c4-fcfe-4d50-aacd-243598ca44e5": {"doc_hash": "9f4d797601e74506edd4ea9e79df1e4f8a7e09cf050da5daaca3937521fd5c80", "ref_doc_id": "aeec18ab-a4cb-4416-8d57-7e4fb4f506f1"}, "f04ca234-b8d6-4e58-b723-0516e6b60137": {"doc_hash": "15d8c1fecf7b0ef97b5e2c2c2f11ddd474057b9b12fb676ee84327a35b780c31", "ref_doc_id": "aeec18ab-a4cb-4416-8d57-7e4fb4f506f1"}, "f37364b8-ee94-4925-8d84-c90e989584a0": {"doc_hash": "d9b8a5ecff937cb7b249ff4f519c35b83158646de7567e022507b9ed05b9d531", "ref_doc_id": "45ca1f90-8479-4a36-b0e4-fd415af66b67"}, "bb6f35e5-8f7a-48c2-8a2a-bcb62b984705": {"doc_hash": "1c43e722353689975dc062973a3d3ebf587ea61f0abb80e3967d1883f6af52e3", "ref_doc_id": "45ca1f90-8479-4a36-b0e4-fd415af66b67"}, "e87e519a-aa20-46e9-b301-2c86036b7aec": {"doc_hash": "2c3a01abe430b66fed77fd466f2634661feb259e955c3bdd0a66a297ef8ad7bc", "ref_doc_id": "09a90510-e7ba-4560-9f63-74eada0d2133"}, "53090279-d6ab-4503-86e1-53d826c9ecc5": {"doc_hash": "97bfd17134f7daecfb2dbffb697812167e0fe129929939f301150c1110ab3875", "ref_doc_id": "09a90510-e7ba-4560-9f63-74eada0d2133"}, "2c3bdc9f-9926-4699-b65c-dc67eb35ce54": {"doc_hash": "ef9a39f20143038946b0f3d6d4ec9cd4993fafb439424b4aa8dd3c9718eac314", "ref_doc_id": "338c2406-61e6-40c5-b0c7-f2d02ce879c8"}, "104c2e46-70ff-4765-ad09-6af12aaa430c": {"doc_hash": "24c0cdccfde8f766b56a36682d4d9e56a8d46441729d32991193d97ceca33d29", "ref_doc_id": "424c989d-da09-43a6-baad-dce8655faafc"}, "a4824e53-e96d-4bfe-bd82-674fca4c6691": {"doc_hash": "b24b392c2216977fe97d86d53be6673e2af59391cba24bf905f9b2c04a75e557", "ref_doc_id": "34b6567e-5b51-47e4-a4ce-89ad91402199"}, "a372b57c-4f76-4ed3-baea-85fc48b5fc7c": {"doc_hash": "a967d062513f1114a7d9d343c0d098db70b68275a08c75ef26023da7d61b151c", "ref_doc_id": "628e0357-e7d6-456e-af9a-e48edd798271"}, "f26abf54-c60c-4847-86f9-7c1658dbffc2": {"doc_hash": "488d2defcd572281bf1694191d015b0517941f8aaf902592b55351f03b9c300a", "ref_doc_id": "d2f7fd52-7267-4f8d-9718-1c265fc8a861"}, "495bc55f-a763-48dc-8a6f-5a75183b1ea1": {"doc_hash": "b1d3378e2b5ea72c4cc7958d19511660c0becf1c98a5faa1c48408ce884458f2", "ref_doc_id": "c32666e6-17d9-4c09-a669-3580c34f3b3e"}, "22941ba0-2d1d-4b5a-87a1-effcbe7304d3": {"doc_hash": "1ed07faf6e1cc417c3c3c7521ceaee6af0e4f1f878a747f17bc07fa374d411fe", "ref_doc_id": "c32666e6-17d9-4c09-a669-3580c34f3b3e"}, "cadb02b5-eb1f-489c-921e-5e10c023b292": {"doc_hash": "d7e5b824b308fc42e7258ad89c56e72bc8e52d90c250e335df3f9c2ff8c578da", "ref_doc_id": "20d250c1-698e-4033-9aec-8a5c398eb4e4"}, "55b9e3b0-3a9d-4738-bf05-78f29808e78b": {"doc_hash": "fc96d77fc52ca22db55c60ce6851909bd39988baf2c5851112855ac277fea35c", "ref_doc_id": "20d250c1-698e-4033-9aec-8a5c398eb4e4"}, "9430c6a7-4f15-4b81-af99-2bc808b89982": {"doc_hash": "9bf353577c1c4db50cd8abf5cbc3fb1c923775f6362611a3ff29e3eb955eed27", "ref_doc_id": "517d8891-51d9-400b-bcb9-77e275ee3c41"}, "ce15688b-a998-43d3-a649-594e34f2a9ea": {"doc_hash": "2c3284ddf275655669d48d2835423f3c99463b6189cf7485bbe5cd5f933078cd", "ref_doc_id": "c30296b3-e35b-4f39-b86f-8bb99f1db9d5"}, "b89fe6cb-a4d8-4637-99f1-921eee8b71f4": {"doc_hash": "9f7b2e543404dbe44ed95ffa8e3ca8c83bc38321c68cbfbb25e323f9acbd594a", "ref_doc_id": "c30296b3-e35b-4f39-b86f-8bb99f1db9d5"}}, "docstore/data": {"d3c7215b-66ac-4da7-b6eb-c74ef1f8d72b": {"__data__": {"id_": "d3c7215b-66ac-4da7-b6eb-c74ef1f8d72b", "embedding": null, "metadata": {"page_label": "1", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0d6e422-0a18-426e-a265-df534c16f15a", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "d990bdcd5c817f7038253efb24d83c7081441e78bee8210c520d89f33eef8da2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Visual Agentic AI for Spatial Reasoning with a Dynamic API\nDamiano Marsili\u2217 Rohun Agrawal\u2217 Yisong Yue Georgia Gkioxari\nCalifornia Institute of Technology\nFigure 1. Spatial reasoning in 3D is challenging as it requires multiple steps of grounding and inference. We introduce a benchmark for\n3D understanding with complex queries; an example is shown here. To tackle these queries we propose a training-free agentic approach,\nV ADAR, that dynamically generates new skills in Python and thus can handle a wider range of queries compared to prior methods.\nAbstract\nVisual reasoning \u2013 the ability to interpret the visual world\n\u2013 is crucial for embodied agents that operate within\nthree-dimensional scenes. Progress in AI has led to vision\nand language models capable of answering questions from\nimages. However, their performance declines when tasked\nwith 3D spatial reasoning. To tackle the complexity of\nsuch reasoning problems, we introduce an agentic program\nsynthesis approach where LLM agents collaboratively\ngenerate a Pythonic API with new functions to solve\ncommon subproblems. Our method overcomes limitations\nof prior approaches that rely on a static, human-defined\nAPI, allowing it to handle a wider range of queries. To\nassess AI capabilities for 3D understanding, we introduce\na new benchmark of queries involving multiple steps\nof grounding and inference. We show that our method\noutperforms prior zero-shot models for visual reason-\ning in 3D and empirically validate the effectiveness of\nour agentic framework for 3D spatial reasoning tasks.\nProject website: https://glab-caltech.github.io/vadar/\n\u2217Equal contribution.\n1. Introduction\nConsider Fig. 1. Here, a person or an agent wants to deter-\nmine the radius of the mirror in the image, given that the\ntable is 20 meters tall. Answering this question requires\nvisual reasoning, a crucial step toward achieving general-\npurpose AI. Visual reasoning enables machines to analyze\nand make sense of the visual world. Humans rely heavily\non visual cues to navigate complex environments, interact\nwith objects and make informed decisions. Our goal is to\nbuild intelligent agents that can do the same. Recent ad-\nvances in AI have produced vision and language models\n(VLMs) [1, 2, 8, 36] that can answer questions from im-\nages. Although impressive, these models excel primarily at\ncategory-level semantic understanding. Their performance\nsignificantly declines when tasked with spatial understand-\ning within the three-dimensional world [6, 19, 38].\nReturning to Fig. 1, to answer the query, an AI agent\nmust first locate the relevant objects, determine their di-\nmensions in pixel space, use their depth to calculate their\n3D sizes, and finally compute the mirror\u2019s radius using the\ntable\u2019s height. This is a complex sequence of tasks, involv-\ning multiple steps of understanding, grounding, and infer-\n1\narXiv:2502.06787v1  [cs.CV]  10 Feb 2025", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2902, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82149970-b769-4e85-ba76-5c82042b38c7": {"__data__": {"id_": "82149970-b769-4e85-ba76-5c82042b38c7", "embedding": null, "metadata": {"page_label": "2", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a95a2aeb-0ecf-4696-a427-3e341b3df175", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "2144068f01ce4ffb6eb03179888cfbd68521eb802370b5afaf3c1700accad21a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41646838-e44b-4f92-b3f9-2352aee3b270", "node_type": "1", "metadata": {}, "hash": "3c78ab2090a736a61729596a25083907ce0e67cb7160fba421c73dbf93e5eadf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ence. GPT4o [1], a state-of-the-art VLM trained on exten-\nsive datasets, gives a wrong final answer.\nTo address the complexity of 3D spatial reasoning tasks,\nwe propose a system of agents working collaboratively to\ncreate executable programs for a given image. Our ap-\nproach leverages LLM agents that dynamically define and\nexpand a domain-specific language (DSL) as needed, gen-\nerating new functions, skills and reasoning, in two phases:\nthe API Generation and the Program Synthesis stage. Vi-\nsion specialists \u2013 an object detector, a depth estimator and\nobject attribute predictor \u2013 help the agents execute the pro-\ngram. We name our approach V ADAR, as it integrates Vi-\nsual, Agentic, Dynamic AI for Reasoning. V ADAR be-\nlongs in the family of visual program synthesis methods,\nlike ViperGPT [35] and VisProg [12], but addresses a key\nlimitation in these approaches: their reliance on a static,\nhuman-defined DSL, which restricts them to a predefined\nrange of functionality. This limitation is evident in Fig. 1,\nwhere ViperGPT generates an incomplete, inaccurate pro-\ngram and VisProg defaults to a holistic visual question an-\nswer (VQA) approach for answering the query. V ADAR\u2019s\noutput in Fig. 1 demonstrates its ability to tackle a wider\nrange of visual queries.\nWe evaluate 3D spatial reasoning using challenging\nbenchmarks designed for rigorous assessment of 3D under-\nstanding. Our evaluation includes CLEVR [18] and our\nnewly introduced benchmark, O MNI 3D-B ENCH , based on\nOmni3D [5]; Fig. 1 shows an example. Both datasets em-\nphasize visual queries involving relative depth, size, and ob-\nject location, often conditioned on measurement hypothe-\nses, requiring grounding and 3D inference. This contrasts\nwith previous spatial reasoning benchmarks like GQA [16],\nwhich primarily emphasize appearance-based reasoning.\nAt a high level, V ADAR roughly mirrors the workflow\nof a software engineer when defining, implement-\ning, and testing new software solutions for a given\nproblem. Leveraging its agentic design, V ADAR au-\ntonomously defines and implements functions such\nas find closest object 3D, is behind,\ncount objects by attributes and position,\nis left of, and more. These functions are used by the\nProgram Agent, resulting in more concise programs, less\noutput tokens and thus a lower likelihood of errors from\nLLM-generated predictions. We empirically show that\nV ADAR outperforms ano-API agent by 6%, highlighting\nthe value of general, reusable, functions within an API.\nMoreover, we show that our generated API significantly\nsurpasses a static, human-defined API used in [12, 35], by\nmore than 20% on CLEVR. V ADAR performs competi-\ntively with state-of-the-art VLMs, on O MNI 3D-B ENCH ,\nwhile also providing executable programs.\nConsidering the rapid progress in AI, one might wonder\nif methods like V ADAR can dominate monolithic VLMs\nin 3D spatial reasoning. One clear advantage of V ADAR\nis its ability to generate interpretable programs. However,\nour experiments highlight another key potential. Improving\nVLMs for 3D reasoning would require extensive datasets\nof image-question-answer tuples with 3D information, an\nonerous endeavor. In contrast, our experiments show that\nif the component vision models \u2013 an object detector, an at-\ntribute predictor and depth estimator \u2013 were replaced with\noracle versions, V ADAR would achieve 83.0% accuracy,\n24% higher from the best VLM. This indicates that V ADAR\nis bottlenecked by the performance of its vision special-\nists. Thus, an alternative path to scaling 3D spatial reason-\ning could be through improving specialized vision models,\nwhich tackle a simpler problem than general-purpose VQA\nand for which training data is more readily available.\n2. Related Work\nOur work draws from areas of language modeling, visual\nprogram synthesis and library learning.\nVLMs for Spatial Reasoning. LLMs [1, 2, 9, 36] are\ntrained on large corpora of text, including domain spe-\ncific languages (DSLs) such as Python. Their multi-modal\nvariants incorporate images and are additionally trained on\nimage-text pairs showing impressive results for visual cap-\ntioning and vision question-answering (VQA) [3].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4171, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41646838-e44b-4f92-b3f9-2352aee3b270": {"__data__": {"id_": "41646838-e44b-4f92-b3f9-2352aee3b270", "embedding": null, "metadata": {"page_label": "2", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a95a2aeb-0ecf-4696-a427-3e341b3df175", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "2144068f01ce4ffb6eb03179888cfbd68521eb802370b5afaf3c1700accad21a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82149970-b769-4e85-ba76-5c82042b38c7", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "86cf5207528de4189a690ccb34612e76973dd4ca80313ee057d8dacbd7f9769b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This indicates that V ADAR\nis bottlenecked by the performance of its vision special-\nists. Thus, an alternative path to scaling 3D spatial reason-\ning could be through improving specialized vision models,\nwhich tackle a simpler problem than general-purpose VQA\nand for which training data is more readily available.\n2. Related Work\nOur work draws from areas of language modeling, visual\nprogram synthesis and library learning.\nVLMs for Spatial Reasoning. LLMs [1, 2, 9, 36] are\ntrained on large corpora of text, including domain spe-\ncific languages (DSLs) such as Python. Their multi-modal\nvariants incorporate images and are additionally trained on\nimage-text pairs showing impressive results for visual cap-\ntioning and vision question-answering (VQA) [3]. De-\nspite their strong performance, their ability to reason be-\nyond category-level semantic queries is limited. Recent\nwork [19, 38] shows that VLMs suffer on visual tasks\nsuch as grounding spatial relationships and inferring object-\ncentric attributes. SpatialRGPT [7] and SpatialVLM [6]\nuse data synthesis pipelines to generate templated queries\nfor spatial understanding. We compare to SpatialVLM and\nshow that it struggles to tackle 3D spatial reasoning queries.\nVisual Program Synthesis. Recent advances in visual rea-\nsoning have led to methods which improve upon the capa-\nbilities of vision-based models by composing them symbol-\nically via program synthesis. VisProg [12] prompts an LLM\nto generate an executable program of a specified DSL that\ncalls and combines vision specialists \u2013 OwlViT [29] for ob-\nject detection, CLIP [32] for classification, and ViLT [21]\nfor VQA. ViperGPT [35] directly generates Python code\nby providing a Python API specification to the LLM agent\nand adds MiDaS [33] as the vision specialist for depth es-\ntimation, in addition to GLIP [25] and X-VLM [45] for\nvision-language tasks. Both approaches rely on a pre-\ndefined DSL, which narrows the scope of applicability and\nmakes these methods difficult to extend to a wider range of\nqueries. Similar to ViperGPT, we use Python as the inter-\nface for our LLM agents, but we don\u2019t define the API a-\npriori. We instead rely on our agentic workflow to generate\nthe API needed to tackle complex spatial reasoning queries.\nWe compare to ViperGPT and VisProg and show that both\n2", "mimetype": "text/plain", "start_char_idx": 3412, "end_char_idx": 5732, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf667c32-3f4f-4795-a7ee-f86577e42b6e": {"__data__": {"id_": "cf667c32-3f4f-4795-a7ee-f86577e42b6e", "embedding": null, "metadata": {"page_label": "3", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dda03ec5-6147-4ed3-88b8-98cb2a2871aa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "41a4cbd58e92e40a635b9dccc36e23f3044f36246db94eaaae511afd41cb40e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8605c42-dee2-4ded-9ffd-a02c49a95f09", "node_type": "1", "metadata": {}, "hash": "9edb3ebc9fddb22f6d9297c953f72c20c6e1cb61fafaacf59ffab37650b5ebff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "struggle to generate accurate programs for complex queries,\noften completely ignoring part of the query.\nLibrary Learning. An emerging field in LLM research\nfocuses on the dynamic creation and extension of a set of\nreusable functions during problem-solving. Early work on\nlibrary learning predates the use of LLMs [10, 23, 39], and\nfocuses on a common architecture of iteratively proposing\nnew programs and synthesizing commonly used compo-\nnents into a library. Modern approaches follow this same\nparadigm, but use LLMs to accelerate the synthesis of use-\nful programs, applied to gaming [40], 3D graphics scripting\n[15], theorem proving [37], and symbolic regression [11].\nNeuro-symbolic AI generates interpretable symbolic com-\nponents for complex tasks and has been explored for a wide\nrange of fields, including spatial reasoning [28], grounding\nof 3D point clouds [13], mechanistic modeling in scientific\ndomains [11, 34], logical reasoning [30], amongst other ar-\neas. Closer to us is the logic-enhanced LLM, LEFT [14],\nthat uses a dynamic DSL of first order logic structures and\ndifferentiably executes them using domain-specific mod-\nules. These modules, instantiated as MLPs, ground spatial\nconcepts, e.g. \u201cis left of\u201d, and are trained with supervision.\nOn CLEVR, V ADAR, which is training-free, achieves the\nsame performance as LEFT when trained with \u2265 10, 000\ntraining samples. A benefit of our training-free approach is\nthat it scales to new domains where 3D supervision is hard\nto acquire, as we show on our OMNI 3D-B ENCH .\nSpatial Reasoning Benchmarks. Existing benchmarks test\naspects of visual reasoning with free-form language [4, 24].\nWe focus on natural-image based ones. VQA [3] introduced\nthe task of visual question answering. GQA [16] is a pop-\nular large-scale VQA benchmark with questions that per-\ntain to object and attribute recognition, of mostly a single-\nstep inference \u2013 \u201cWhat color is the cat next to the chair?\u201d,\n\u201cWhat type of vehicle is on top of the road?\u201d, \u201cDo the wild-\nflowers look ugly?\u201d. RefCOCO [20] targets object localiza-\ntion with referring expressions such as \u201cthe man in a red\nshirt\u201d. What\u2019s up [19] quantifies comprehension of basic\n2D spatial relations such as \u201cleft of\u201d and \u201cabove\u201d. These\nbenchmarks evaluate aspects of visual reasoning, but criti-\ncally omit 3D understanding. Q-Spatial Bench [26] focuses\nsolely on absolute 3D measurements. Cambrian-1 [38] pro-\nposes a VQA benchmark repurposing images and annota-\ntions from Omni3D [5], but its queries focus on the relative\ndepth and depth ordering of objects with (2 or 3)-choice\nquestions. Our benchmark also repurposes Omni3D anno-\ntations, but in contrast to Cambrian-1, we design more com-\nplex queries that extend beyond depth ordering and multiple\nchoice. Concurrent to our work, VSI-Bench [44] introduces\na video understanding benchmark focused on spatial rela-\ntionships, which we discuss extensively in Appendix D.\n3. Method\nAt the core of our approach is a dynamic API generated\nby LLMs that can be extended to address new queries that\nrequire novel skills. The goal of the API is to break down\ncomplex reasoning problems into simpler subproblems with\ngeneral modules that can be used during program synthesis.\nOur approach consists of an API Generation stage and a\nProgram Synthesis stage, illustrated in Fig. 2.\nVision Specialists. During program execution on the im-\nage, we employ vision models for solving visual subtasks:\nMolmo\u2019s [8] pointing model and GroundingDINO [27]\nare used to localize objects prompted with text ( loc),\nSAM [22] returns the bounding box from the object\u2019s mask\nprompted with Molmo\u2019s points (get 2D object size),\nUniDepth [31] estimates the depth at an image location\n(depth), GPT4o is utilized as a VQA module to query ob-\nject attributes (color, material) from an image with the target\nobject bounding box overlayed (vqa). We initialize the API\nwith these functions. The API also includessame object\nthat computes the overlap of two object bounding boxes to\ndetermine if the objects are the same.\n3.1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4050, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8605c42-dee2-4ded-9ffd-a02c49a95f09": {"__data__": {"id_": "a8605c42-dee2-4ded-9ffd-a02c49a95f09", "embedding": null, "metadata": {"page_label": "3", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dda03ec5-6147-4ed3-88b8-98cb2a2871aa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "41a4cbd58e92e40a635b9dccc36e23f3044f36246db94eaaae511afd41cb40e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf667c32-3f4f-4795-a7ee-f86577e42b6e", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "7d0410d5d7b1108d80c1bfb17a7ed172213ed47e2466768e63d93d0c63d7ae8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Our approach consists of an API Generation stage and a\nProgram Synthesis stage, illustrated in Fig. 2.\nVision Specialists. During program execution on the im-\nage, we employ vision models for solving visual subtasks:\nMolmo\u2019s [8] pointing model and GroundingDINO [27]\nare used to localize objects prompted with text ( loc),\nSAM [22] returns the bounding box from the object\u2019s mask\nprompted with Molmo\u2019s points (get 2D object size),\nUniDepth [31] estimates the depth at an image location\n(depth), GPT4o is utilized as a VQA module to query ob-\nject attributes (color, material) from an image with the target\nobject bounding box overlayed (vqa). We initialize the API\nwith these functions. The API also includessame object\nthat computes the overlap of two object bounding boxes to\ndetermine if the objects are the same.\n3.1. API Generation\nAlgorithm 1: V ADAR: API Generation\nData: Questions Q\nS \u2190 {} // Signatures\nA \u2190 {Vision Models} // API Methods\nfor batch B \u2282 Qdo\nS \u2190 S \u222aSignatureAgent(B)\nend\nfor S \u2208 Sdo\neS \u2190 0 // Error count\nA \u2190 ImplementationAgent(S)\nE \u2190 TestAgent(A)\nif Python Exception E then\nif eS = 5then continue\nelse if E is \u201cundefined method U\u201d then\neS \u2190 eS + 1\nRecursively implement U\nelse\neS \u2190 eS + 1\nRe-implement S using E\nend\nelse\nA \u2190 A \u222aA\nend\nend\nreturn A\nAlgorithm 1 describes the API Generation. Here, the\nSignature Agent and the Implementation Agent collab-\norate to define and implement new functions as needed to\n3", "mimetype": "text/plain", "start_char_idx": 3229, "end_char_idx": 4664, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bf575bae-5bde-455a-bc2d-98af1b61d0f8": {"__data__": {"id_": "bf575bae-5bde-455a-bc2d-98af1b61d0f8", "embedding": null, "metadata": {"page_label": "4", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "318c1074-7f40-42ba-a51e-22e18e141ea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "cc10784d928a8d948418aba585db10065636ca59244ba4d791f6654923829467", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 2. Overview. V ADAR consists of an API generation stage and a program synthesis stage. The Signature & Implementation Agents\ngenerate an API that is used by the Program Agent to produce a program to answer the question, executed by the Execution Agent.\naid in solving the queries. First, the Signature Agent re-\nceives a batch of N queries (N = 15), without answers,\nand is instructed to produce general method signatures for\nsubproblems that could arise when answering those kinds\nof queries. The Implementation Agent then implements the\nsignatures in Python. Examples of signatures and their im-\nplementations are shown in Fig. 2.\nPrompting the Signature Agent. The agent receives the cur-\nrent API state as docstrings so it avoids duplicating existing\nmethods. We observed that our Signature Agent performed\nbetter without in-context examples as it produced a more\ndiverse API with wider potential functionality.\nPrompting the Implementation Agent. The Implementation\nAgent receives all other signatures in the API along with\nthe signature it needs to implement, so it can use other\nAPI methods in its implementation, enabling a hierarchy\nin the API. In contrast to the Signature Agent, providing\nin-context examples significantly enhances the Implemen-\ntation Agent\u2019s output, as implementation prioritizes accu-\nracy over diversity. We refer to these examples as weak\nin-context learning (ICL), as they guide correct method im-\nplementation in Python, unlike strong ICL, which breaks\ndown queries into full programs. Prompts for both agents\nand weak-ICL examples are found in the Appendix.\nDepth-First Implementation . Once a method is imple-\nmented from its signature, the Test Agent, a Python inter-\npreter, runs it using placeholder inputs. If a runtime error\noccurs, the Test Agent signals the Implementation Agent to\nrevise it with the exception message. However, if the im-\nplementation relies on another yet-to-be-implemented API\nmethod, the test run cannot proceed. In this case, the Imple-\nmentation Agent traverses an implicit dependency graph,\ndepth-first, ensuring that prerequisite methods are imple-\nmented first (see Algo. 1).\nConsider the following example where the sig-\nnatures get color, find objects by color,\ncount objects left of, and is left of, are\ndefined by the Signature Agent, in that order. First, the\nImplementation Agent will implement get color, the\nTest Agent will be called, and barring no runtime errors,\nthe method will be complete. Then, the implementation\nfor find objects by color uses get color, which\nis implemented, so the Test Agent only checks for Python\nerrors. If count objects left of attempts to use\nis left of, the Test Agent will detect thatis left of\nis not implemented and recursively call the Implemen-\ntation Agent to implement is left of, followed by\ncount objects left of.\nIn the event a cycle in the dependency graph is persis-\ntent after attempting the implementation of those methods 5\ntimes, the methods in the cycle are deleted. Empirically, we\nrarely detect such cycles, which can be attributed to the Sig-\nnature Agent producing multiple signatures at once, tending\nto avoid proposing signatures that overlap in function.\n3.2. Program Synthesis\nThe Program Agent receives the generated API and a sin-\ngle question as input. Its task is to generate Python code\nthat leverages the API to solve the question. The Execu-\ntion Agent, another Python interpreter, executes the pro-\ngram line-by-line. In the event of a Python error, it provides\nthe Program Agent with the exception, and a new program\nis generated. This is repeated at most 5 times, after which\nthe program returns an execution error.\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3669, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90e95fad-8171-4790-8503-14359a2cf289": {"__data__": {"id_": "90e95fad-8171-4790-8503-14359a2cf289", "embedding": null, "metadata": {"page_label": "5", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a975e2b-6fd7-4f9b-9ef5-1c5f0557fde5", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "1dd86cf31931a7d92875a00adc6b39f28c4dfbbec8a5b60ce1f4a186baead904", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f262115b-e579-4fe6-a3f3-b872c05747b6", "node_type": "1", "metadata": {}, "hash": "f983165a60f1af3b49314860ed33e367053aee5a78b501e5c52828503e42c727", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Algorithm 2: V ADAR: Program Synthesis\nData: Image-Query pairs D = {(I, Q)},\nAPI methods A\nR \u2190 {} // Results\nfor (I, Q) \u2208 Ddo\neP \u2190 0 // Error count\nP \u2190 ProgramAgent(Q, A)\nE, R\u2190\nExecutionAgent(P, I,Vision Models)\nif Python Exception E and eP < 5 then\neP \u2190 eP + 1\nRe-generate P using E\nelse\nR \u2190 R \u222aR\nend\nend\nreturn R\nPrompting the Program Agent. Following the success of\nChain-of-Thought (CoT) prompting [41], we instruct the\nProgram Agent to create a plan before generating the corre-\nsponding program. In-context examples boost the Program\nAgent\u2019s performance. However, unlike VisProg [12] and\nViperGPT [35] that use strong-ICL, we use API-agnostic\nnatural language instructions since the API is not prede-\nfined, making it impossible to provide full program exam-\nples. These instructions help for the same reason as with the\nImplementation Agent, to focus on correctness. The prompt\nfor the Program Agent is provided in the Appendix.\nTest & Execution Agent vs Critics. In modern library learn-\ning, LLM agents, or critics, evaluate the quality and util-\nity of learned functions. Our Test and Execution Agents\nalso assess method quality, but we opt for deterministic crit-\nics that leverage the full Python runtime, signaling LLM\nAgents with Python exceptions in case of errors.\n4. Experiments\nWe conduct experiments on challenging spatial reason-\ning benchmarks and demonstrate the effectiveness of a\ndynamically generated API by LLM agents compared to\na static, human-defined API in ViperGPT [35] and Vis-\nProg [12], which we outperform by a large margin.\nWe also compare to monolithic state-of-the-art VLMs,\ntrained on billions of (image, question, answer) samples,\nand show that our approach competes favorably and even\nsurpasses them on certain question types while also provid-\ning interpretable reasoning steps to complex queries.\n4.1. A Benchmark for Spatial Reasoning in 3D\nWe evaluate 3D spatial reasoning using CLEVR, and our\nnewly introduced benchmark, OMNI 3D-B ENCH .\nCLEVR [18] consists of (image, question, answer) tuples.\nEach image contains 2-10 objects of 3 different shapes, 8\ncolors, 2 materials, and 2 sizes. Despite the simplicity of the\nscenes, the questions in CLEVR are complex, e.g., \u201cThere\nis a large ball right of the large metal sphere that is left\nof the large object that is behind the small brown sphere;\nwhat color is it?\u201d. Our CLEVR benchmark contains 1,155\nsamples, 400 of which require a numerical answer, 399 are\nyes/no questions, and 356 are multiple-choice questions.\nOMNI 3D-B ENCH is sourced from Omni3D [5], a dataset\nof images from diverse real-world scenes with 3D object\nannotations. We repurpose images from Omni3D to a VQA\nbenchmark, with questions about 3D information portrayed\nin the image, such as\u201cIf the height of the front most chair is\n6 meters in 3D, what is the height in 3D of the table in the\nimage?\u201d and \u201cHow many bottles would you have to stack\non top of each other to make a structure as tall in 3D as\nthe armchair?\u201d. O MNI 3D-B ENCH complements CLEVR\nwith non-templated queries pertaining to 3D locations and\nsizes of objects. Our queries test 3D reasoning, as they re-\nquire grounding objects in 3D and combining predicted at-\ntributes to reason about distances and dimensions in three\ndimensions. O MNI 3D-B ENCH consists of 500 extremely\nchallenging (image, question, answer) tuples.\nWe compare our proposed benchmark to GQA [16], a\npopular visual reasoning dataset. GQA derives queries from\nscene graphs which primarily pertain to the visual appear-\nance and attributes of objects. Example queries in GQA\nare \u201cIs there a red truck or bus?\u201d , \u201cIs the field short and\nbrown?\u201d and \u201cIs the chair in the top part of the image?\u201d .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f262115b-e579-4fe6-a3f3-b872c05747b6": {"__data__": {"id_": "f262115b-e579-4fe6-a3f3-b872c05747b6", "embedding": null, "metadata": {"page_label": "5", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a975e2b-6fd7-4f9b-9ef5-1c5f0557fde5", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "1dd86cf31931a7d92875a00adc6b39f28c4dfbbec8a5b60ce1f4a186baead904", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90e95fad-8171-4790-8503-14359a2cf289", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "f3dec50daef175f65991a21c5b947f78b5a227b4a76242f876d6ddfd76067178", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "O MNI 3D-B ENCH complements CLEVR\nwith non-templated queries pertaining to 3D locations and\nsizes of objects. Our queries test 3D reasoning, as they re-\nquire grounding objects in 3D and combining predicted at-\ntributes to reason about distances and dimensions in three\ndimensions. O MNI 3D-B ENCH consists of 500 extremely\nchallenging (image, question, answer) tuples.\nWe compare our proposed benchmark to GQA [16], a\npopular visual reasoning dataset. GQA derives queries from\nscene graphs which primarily pertain to the visual appear-\nance and attributes of objects. Example queries in GQA\nare \u201cIs there a red truck or bus?\u201d , \u201cIs the field short and\nbrown?\u201d and \u201cIs the chair in the top part of the image?\u201d .\nThese are significantly simpler to queries in CLEVR and\nOMNI 3D-B ENCH which involve multiple steps of ground-\ning and inference in two- and three- dimensions.\n4.2. Results on Spatial Reasoning in 3D\nTab. 1 compares our approach, V ADAR, to state-of-the-art\nVLMs and Program Synthesis methods. Fig. 3 additionally\ncompares to the neuro-symbolic LEFT [14]. V ADAR uses\nGPT4o with a temperature of 0.7 for all agents.\nVLMs vs V ADAR. VLMs, such as GPT4o [1], Claude-\nSonnet [2], Gemini [36], Llama3.2-11B [9], and Molmo-\n7B [8], are monolithic models trained on vast of image-\nquestion-answer datasets, likely including samples with\nspatial and 3D information. We expect them to perform\nwell on related tasks. We also compare to SpaceMan-\ntis [6, 17], the most recent and largest SpatialVLM [6]\nvariant, finetuned on data with 3D information. We an-\nalyze performance based on three answer types: yes/no,\nmultiple-choice and numerical answers. For queries with\nfloating point answers, we report MRA [44] with thresholds\nC = {0.5, 0.55, ...,0.95} for outputs \u02c6y and ground truth y:\nMRA = 1\n|C|\nP\n\u03b8\u2208C 1\n\u0012\n|\u02c6y\u2212y|\ny < 1 \u2212 \u03b8\n\u0013\n5", "mimetype": "text/plain", "start_char_idx": 2982, "end_char_idx": 4815, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2a8683b-3c7b-4a67-af60-996befea0c43": {"__data__": {"id_": "e2a8683b-3c7b-4a67-af60-996befea0c43", "embedding": null, "metadata": {"page_label": "6", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e78b880-2bf8-4a86-87e0-d102570ee179", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "3f6e12b3d60d2a14a8f018c163be5f38e331da654956b6fa3a89f0f50a3f5b6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "775dae96-69c0-4a03-ab6e-5d490ea914ea", "node_type": "1", "metadata": {}, "hash": "1ace4abdb1b5c967fbb08dc538215bf00bc4e1edfbcf2c1734faea0a0690e946", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CLEVR OMNI3D-BENCH\nnumeric y/n multi-choice Total numeric (ct) numeric (other) y/n multi-choice Total\nVLMs\nGPT4o [1] 52.3 63.0 60.0 58.4 28.1 35.5 66.7 57.2 42.9\nClaude3.5-Sonnet [2] 44.7 61.4 72.2 58.9 22.4 20.6 62.2 50.6 32.2\nLlama3.2 [9] 34.6 45.6 49.0 42.8 24.3 19.3 47.5 27.4 25.6\nGemini1.5-Pro [36] 44.9 59.7 67.0 56.9 25.2 28.1 46.2 37.6 32.0\nGemini1.5-Flash [36] 43.1 58.8 56.8 52.8 24.3 27.6 51.1 52.9 35.0\nMolmo [8] 11.0 42.6 51.4 34.4 21.4 21.7 29.3 41.2 26.1\nSpaceMantis [6, 17] 14.5 52.9 32.3 33.2 20.0 21.7 50.6 48.2 30.3\nProgram\nSynthesis\nViperGPT [35] 20.5 43.4 13.4 26.2 20.0 15.4 56.0 42.4 26.7\nVisProg [12] 16.7 48.4 28.3 31.2 2.9 0.9 54.7 25.9 13.5\nV ADAR (ours) 53.3 65.3 40.8 53.6 21.7 35.5 56.0 57.6 40.4\nTable 1. Accuracy (%) on CLEVR and O MNI 3D-B ENCH . We compare to state-of-the-art monolithic VLMs and Program Synthesis\napproaches. For each benchmark, we breakdown performance for numeric (ct), numeric (other), yes/no and multiple-choice answers and\nreport total accuracy. For numeric (other) queries, which require floating point answers, we report MRA. V ADAR outpeforms ViperGPT\nand VisProg with a big margin. V ADAR outperforms all large VLMs on OMNI 3D-B ENCH except GPT4o, which it is narrowly behind.\nCLEVR OMNI3D-BENCH\nnumeric y/n multi-choice Total numeric (ct) numeric (other) y/n multi-choice Total\nViperGPT [35] 38.5 57.8 30.2 42.6 50.0 17.8 66.7 49.3 54.9\nVisProg [12] 25.3 52.5 41.8 39.9 100.0 23.5 68.5 66.7 66.0\nV ADAR (ours) 82.4 85.4 81.0 83.0 100.0 82.3 100.0 94.1 94.4\nTable 2. Oracle accuracy (%) on CLEVR and O MNI 3D-B ENCH . We evaluate program correctness by replacing vision specialists with\noracle variants. V ADAR\u2019s high oracle accuracy indicates its main limitation is the vision specialists\u2019 performance.\nFigure 3. LEFT [14] vs V ADAR on CLEVR.LEFT requires su-\npervision. We vary the amount of training data (x-axis) and report\naccuracy (y-axis). V ADAR requiresno supervision but takes in 15\nqueries without answers to guide the creation of the API. V ADAR\noutperforms LEFT trained with \u2264 10, 000 supervised examples.\nFrom Tab.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2090, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "775dae96-69c0-4a03-ab6e-5d490ea914ea": {"__data__": {"id_": "775dae96-69c0-4a03-ab6e-5d490ea914ea", "embedding": null, "metadata": {"page_label": "6", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e78b880-2bf8-4a86-87e0-d102570ee179", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "3f6e12b3d60d2a14a8f018c163be5f38e331da654956b6fa3a89f0f50a3f5b6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2a8683b-3c7b-4a67-af60-996befea0c43", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "e037a06c35ac3b6c09731401bdab29e0253b09acd892d118bb3a183bf572f44f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Oracle accuracy (%) on CLEVR and O MNI 3D-B ENCH . We evaluate program correctness by replacing vision specialists with\noracle variants. V ADAR\u2019s high oracle accuracy indicates its main limitation is the vision specialists\u2019 performance.\nFigure 3. LEFT [14] vs V ADAR on CLEVR.LEFT requires su-\npervision. We vary the amount of training data (x-axis) and report\naccuracy (y-axis). V ADAR requiresno supervision but takes in 15\nqueries without answers to guide the creation of the API. V ADAR\noutperforms LEFT trained with \u2264 10, 000 supervised examples.\nFrom Tab. 1, we observe that on CLEVR, GPT4o,\nClaude-Sonnet, and Gemini perform best on average\nwhile V ADAR slightly outperforms VLMs on numeric\n(by 1.0%) and yes/no answers (by 2.3%). On the chal-\nlenging O MNI 3D-B ENCH , V ADAR is behind GPT4o by\njust 2% and outperforms all other VLMs by more than\n5%. Llama3.2-11B and Molmo-7B perform worse among\nVLMs likely due to their smaller size.\nViperGPT vs VisProg vs V ADAR. V ADAR outperforms\nboth models on both CLEVR and O MNI 3D-B ENCH by\nmore than 20%. VisProg and V ADAR use GPT4o as their\nLLM; ViperGPT uses GPT-3.5 as it performed better.\nSeparating program correctness from execution accu-\nracy, Tab. 2 provides comparisons to ViperGPT and Vis-\nProg when vision specialists are replaced with oracle ones.\nOn CLEVR, we use an Oracle Execution Agent that lever-\nages the true scene annotations to provide the correct out-\nput automatically. For OMNI 3D-B ENCH , we manually ver-\nified program correctness as ground truth 3D information\nis not available for all objects in the scene. The results re-\nveal that with oracle vision specialists, V ADAR achieves an\naccuracy of 83.0% on CLEVR and 94.4% on O MNI 3D-\nBENCH , compared to ViperGPT\u2019s 42.6% and 54.9%, and\nVisProg\u2019s 39.9% and 66.0% respectively. This suggests\nthat our approach can handle a significantly wider variety\nof queries, thanks to the dynamically generated API created\nby our LLM agents, as opposed to the static, human-defined\nAPI used in ViperGPT and VisProg. Our API is simpler and\nallows for flexible integration of vision specialists, avoiding\nthe biases introduced by humans \u2013e.g., as in VisProg, where\nthe pre-defined API guides the LLM to define \u201dbehind\u201d by\ncropping the image above.\nThe high accuracy of V ADAR with oracle vision special-\nists suggests a promising path to scaling 3D spatial reason-\ning: improving specialized vision models. These models\nare easier to train than general-purpose VLMs, as they ad-\ndress simpler tasks and have more accessible training data.\nFig. 4 shows programs generated by the methods. We\nobserve that ViperGPT and VisProg tend to resort to direct\nVQA calls when questions are complex, as opposed to gen-\nerating programs. In addition, ViperGPT often tends to pro-\nduce incomplete programs, ignoring a significant portion of\nthe query. Finally, both ViperGPT and VisProg often con-\nfuse above-behind and below-in front. This seems to be a\n6", "mimetype": "text/plain", "start_char_idx": 1529, "end_char_idx": 4487, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e51191d0-7aaf-4d55-a687-292c456ac936": {"__data__": {"id_": "e51191d0-7aaf-4d55-a687-292c456ac936", "embedding": null, "metadata": {"page_label": "7", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "41e9d02f-7d9e-4dd2-85f7-8cd3cf48e2fc", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "2d76ade7477d4929d187e1e398165bfadf958114e5a357dea1038194325f7c3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 4. Program outputs for VisProg, ViperGPT and V ADAR. For each example, we show the query, the input image, and the\nmethod\u2019s program generations. Queries are from our benchmark and pertain to 3D understanding of scenes. Zoom-in to read the programs.\nsemantic error for ViperGPT that uses a depth estimation\nmodule, like us, and a conceptual design error by VisProg\nthat implements CROP BEHIND to crop above in the image.\nLEFT [14] vs V ADAR. We also compare to the logic-\nenhanced neuro-symbolic approach LEFT [14], which uses\ntrained modules to ground visual concepts in images, such\nas \u201c is left of\u201d. Unlike LEFT, our approach is en-\ntirely training-free, while LEFT requires extensive super-\nvision for module training. Fig. 3 reports the performance\nof LEFT on the CLEVR dataset when trained (to conver-\ngence) with varying training set sizes (x-axis). Although\nour approach does not require any explicit supervision, our\nAPI agent uses a small sample ( = 15) of questions only,\nwithout answers, to construct the API. According to Fig. 3,\nwe outperform LEFT trained with \u2264 10, 000 examples on\nCLEVR. Notably, it is not possible to evaluate LEFT on\nOMNI 3D-B ENCH due to its reliance on a large, domain-\nspecific training set with appropriate 3D supervision, which\nis difficult to obtain for this benchmark or in general. This\nhighlights an added advantage of our method: its ability to\nscale to new domains without the need for training.\nResults on GQA. We report results on GQA [16], a widely\nused benchmark for spatial reasoning. As noted earlier,\nGQA queries emphasize object appearance and attributes,\nand primarily require one-step inference. Questions in\nGQA include \u201cWhat size is the doughnut the person is eat-\ning?\u201d and \u201cWho is sitting in front of the water?\u201d . Tab. 3\ncompares GPT4o, ViperGPT, VisProg, and V ADAR. We\nobserve different relative model performance compared to\nTab. 1. Given the nature of GQA, it is not surprising that\na monolithic and performant VLM like GPT4o would per-\nform well, which our results confirm. Among the program\nsynthesis methods, we observe that V ADAR and VisProg\nachieve comparable performance, while ViperGPT shows a\ndrop in accuracy. A deeper dive into the output programs\nshows that VisProg relies on image-wide VQA calls in 34%\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2288, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a82810d6-3525-4e78-9488-075619653cb5": {"__data__": {"id_": "a82810d6-3525-4e78-9488-075619653cb5", "embedding": null, "metadata": {"page_label": "8", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce758829-0c38-406c-9d34-99cf3b9cfb7a", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "9cac35e5d04ccc3e4a798e2b5da7c40ed3f71794f8fbb54ba234a4a57130ef57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e64cfe7-17fa-4099-9eca-1706ab9a40cf", "node_type": "1", "metadata": {}, "hash": "67f34dfac428b10d28129d78b0336d8796856536cc4f57ccb0f6bd2a223575c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Method GQA\nGPT4o [1] 54.9\nViperGPT [35] 42.0\nVisProg [12] 46.9\nV ADAR (ours) 46.1\nTable 3. Results on GQA on a subset of testdev split. GQA fo-\ncuses primarily on object appearance, not 3D spatial reasoning.\nCLEVR 100\nNo-API Agent 60.7\nAPI Agent 64.0\n+ Weak ICL 65.7\n+ Pseudo ICL 66.7\nTable 4. Ablations of agentic design and prompts on CLEVR\n100, a subset of 100 questions. We compare to single agent variant\nNo-API which creates programs directly. We then ablate prompt-\ning by incrementally adding instructions to the agents used to\ndefine the API. The No-API Agent performs the worst and our\nprompting techniques add to V ADAR\u2019s performance.\nof cases, whereas V ADAR does so only 24% of the time.\nThe limitations of GQA queries in evaluating 3D spatial\nreasoning highlight the need for our proposed benchmark,\nwhich better assesses 3D understanding and exposes the\nweaknesses of current methods.\n4.3. Ablations\nWe turn to ablations to quantify the effectiveness of the\nagentic design and prompting in our approach. To reduce\ncosts from GPT4o, we experiment on a randomly selected\nCLEVR subset. Tab. 4 compares the following variants:\nNo-API Agent is a single agent instructed to directly create\nprograms for queries without defining an API of reusable\nmethods. Comparison to this variant shows the value of an\nAPI. Fig. 5 shows a common reasoning error by theNo-API\nAgent, which confuses depth with left/right; our approach,\nby implementing reusable methods, invokes the appropri-\nately named method that is accurately implemented. The\nexample reiterates that spatial reasoning relies on correct-\nness, supporting V ADAR\u2019s design to build an accurate API\nbefore program synthesis, over library learning, that discov-\ners a potentially incorrect library after program synthesis.\nAPI Agent is our approach without any prompting instruc-\ntions or ICL examples. We incrementally add our two\nprompting techniques: (1) Weak ICL examples guide the\nImplementation Agent to use the pre-defined modules. (2)\nPseudo ICL provides pseudo-code examples and instruc-\ntions in natural language to the Implementation and Pro-\ngram Agent, respectively, that demonstrate how to handle\nintricate queries. We provide the prompts in the Appendix.\nFrom Tab. 4 we observe that the No-API Agent performs\nthe worst, while our prompting techniques via weak ICL\nexamples and instructions achieve the best performance.\n(a) No-API Agent (b) V ADAR\nFigure 5. (a) The No-API agent produces longer programs and is\nprone to errors, often mistakenly using depth for left/right com-\nparisons. (b) In contrast, our agentic V ADAR creates shorter pro-\ngrams by leveraging methods from the API.\n5. Limitations & Future Work\nWe introduce V ADAR, an agentic approach that leverages\nLLM agents to dynamically create and expand a Pythonic\nAPI for complex 3D visual reasoning tasks. Our agents au-\ntonomously generate and implement functions, which are\nthen utilized by the Program Agent to produce programs.\nThis reuse of functions results in more accurate programs\nfor complex queries. There is an extensive list of future di-\nrections to address current limitations of V ADAR.\n\u2022 V ADAR often struggles with queries that require 5 or\nmore inference steps, e.g. \u201cThere is a yellow cylinder to\nthe right of the cube that is behind the purple block; is\nthere a brown object in front of it?\u201d. We provide the pro-\ngrams for these complex cases in the Appendix. Address-\ning such queries can be improved by leveraging advanced\nprompting strategies, an active research area that includes\nmethods like CoT [41] and prompt chaining [42, 43].\n\u2022 We show that V ADAR attains high program accuracy\n(e.g., 83.0% on CLEVR) but lower execution accuracy\n(53.6%) due to errors from the vision specialists. A po-\ntential enhancement would be to enable V ADAR to dy-\nnamically choose its vision modules from a pool of avail-\nable options based on empirical performance. Integrating\nthe selection process with reinforcement learning or self-\nimprovement mechanisms is a promising future direction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4038, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e64cfe7-17fa-4099-9eca-1706ab9a40cf": {"__data__": {"id_": "6e64cfe7-17fa-4099-9eca-1706ab9a40cf", "embedding": null, "metadata": {"page_label": "8", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce758829-0c38-406c-9d34-99cf3b9cfb7a", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "9cac35e5d04ccc3e4a798e2b5da7c40ed3f71794f8fbb54ba234a4a57130ef57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a82810d6-3525-4e78-9488-075619653cb5", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "65f35c9dfc5e427797be1c3e3cf2111ecfbcdb4a66890e0d60f7e9970ab6e097", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u201cThere is a yellow cylinder to\nthe right of the cube that is behind the purple block; is\nthere a brown object in front of it?\u201d. We provide the pro-\ngrams for these complex cases in the Appendix. Address-\ning such queries can be improved by leveraging advanced\nprompting strategies, an active research area that includes\nmethods like CoT [41] and prompt chaining [42, 43].\n\u2022 We show that V ADAR attains high program accuracy\n(e.g., 83.0% on CLEVR) but lower execution accuracy\n(53.6%) due to errors from the vision specialists. A po-\ntential enhancement would be to enable V ADAR to dy-\nnamically choose its vision modules from a pool of avail-\nable options based on empirical performance. Integrating\nthe selection process with reinforcement learning or self-\nimprovement mechanisms is a promising future direction.\n\u2022 V ADAR creates a program based solely on the input\nquery, utilizing the image only during execution. Incorpo-\nrating the image into the program synthesis process could\nimprove accuracy, potentially improving performance on\nqueries requiring five or more inference steps.\nWe release the benchmark and code for V ADAR to fos-\nter future research in this direction.\n8", "mimetype": "text/plain", "start_char_idx": 3223, "end_char_idx": 4405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2bed3b4-3c8c-407c-a4d6-2183baba9fee": {"__data__": {"id_": "c2bed3b4-3c8c-407c-a4d6-2183baba9fee", "embedding": null, "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4bc12972-10b0-4afb-82d6-61ffa14b87b1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "b06e04bc65f080c1caf8e830bf95329d1e471798d0ac37fef0fdfe7f8200b6f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e9fc9c4-6a8f-4ad9-b09d-fd1f6742f464", "node_type": "1", "metadata": {}, "hash": "a6fb1c7aa19840bfb8fbc626e55eff7588a5cdeb9b0b37c11007bb1845c913a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Acknowledgments\nThe project is funded by Meta through the LLM evalua-\ntion research grant and partly through Caltech\u2019s CAST pro-\ngram. We also thank Google\u2019s Gemma Academic program\nand OpenAI for granting us API credits for their LLMs.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report. arXiv preprint arXiv:2303.08774 ,\n2023. 1, 2, 5, 6, 8\n[2] Anthropic. Claude, 2024. 1, 2, 5, 6\n[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.\nVqa: Visual question answering. In ICCV, 2015. 2, 3\n[4] ARC-AGI. Arc prize, 2024. 3\n[5] Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi,\nJustin Johnson, and Georgia Gkioxari. Omni3D: A large\nbenchmark and model for 3D object detection in the wild.\nIn CVPR, 2023. 2, 3, 5, 1\n[6] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa\nSadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endow-\ning vision-language models with spatial reasoning capabili-\nties. In CVPR, 2024. 1, 2, 5, 6\n[7] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Rui-\nhan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatial-\nrgpt: Grounded spatial reasoning in vision-language models.\nIn NeurIPS, 2024. 2\n[8] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tri-\npathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi,\nNiklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo\nand pixmo: Open weights and open data for state-of-the-art\nmultimodal models. arXiv preprint arXiv:2409.17146, 2024.\n1, 3, 5, 6\n[9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-\nhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The\nllama 3 herd of models. arXiv preprint arXiv:2407.21783 ,\n2024. 2, 5, 6, 1\n[10] Kevin Ellis, Lionel Wong, Maxwell Nye, Mathias Sable-\nMeyer, Luc Cary, Lore Anaya Pozo, Luke Hewitt, Armando\nSolar-Lezama, and Joshua B Tenenbaum. Dreamcoder:\ngrowing generalizable, interpretable knowledge with wake\u2013\nsleep bayesian program learning. Philosophical Transac-\ntions of the Royal Society A, 2023. 3\n[11] Arya Grayeli, Atharva Sehgal, Omar Costilla-Reyes, Miles\nCranmer, and Swarat Chaudhuri. Symbolic regression with\na learned concept library. In NeurIPS, 2024. 3\n[12] Tanmay Gupta and Aniruddha Kembhavi. Visual program-\nming: Compositional visual reasoning without training. In\nCVPR, 2023. 2, 5, 6, 8, 1\n[13] Joy Hsu, Jiayuan Mao, and Jiajun Wu. Ns3d: Neuro-\nsymbolic grounding of 3d objects and relations. In CVPR,\n2023. 3\n[14] Joy Hsu, Jiayuan Mao, Josh Tenenbaum, and Jiajun Wu.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2686, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e9fc9c4-6a8f-4ad9-b09d-fd1f6742f464": {"__data__": {"id_": "0e9fc9c4-6a8f-4ad9-b09d-fd1f6742f464", "embedding": null, "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4bc12972-10b0-4afb-82d6-61ffa14b87b1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "b06e04bc65f080c1caf8e830bf95329d1e471798d0ac37fef0fdfe7f8200b6f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2bed3b4-3c8c-407c-a4d6-2183baba9fee", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "87239282ab435fedfb70fc0ad9482eb9ed28d8606362aeced818c497e87dcc88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5aa2686f-712f-48d0-95fa-a9ccac8876f1", "node_type": "1", "metadata": {}, "hash": "9914d5db96a2d7ab772cb25b660f7a0a9308bb06e863208f42b2b4ad28a6faab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3\n[11] Arya Grayeli, Atharva Sehgal, Omar Costilla-Reyes, Miles\nCranmer, and Swarat Chaudhuri. Symbolic regression with\na learned concept library. In NeurIPS, 2024. 3\n[12] Tanmay Gupta and Aniruddha Kembhavi. Visual program-\nming: Compositional visual reasoning without training. In\nCVPR, 2023. 2, 5, 6, 8, 1\n[13] Joy Hsu, Jiayuan Mao, and Jiajun Wu. Ns3d: Neuro-\nsymbolic grounding of 3d objects and relations. In CVPR,\n2023. 3\n[14] Joy Hsu, Jiayuan Mao, Josh Tenenbaum, and Jiajun Wu.\nWhat\u2019s left? concept grounding with logic-enhanced foun-\ndation models. In NeurIPS, 2024. 3, 5, 6, 7\n[15] Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong\nYue, David A Ross, Cordelia Schmid, and Alireza Fathi.\nScenecraft: An llm agent for synthesizing 3d scenes as\nblender code. In ICML, 2024. 3\n[16] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In CVPR, 2019. 2, 3, 5, 7\n[17] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku,\nQian Liu, and Wenhu Chen. Mantis: Interleaved multi-image\ninstruction tuning, 2024. 5, 6, 1\n[18] Justin Johnson, Bharath Hariharan, Laurens Van\nDer Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross\nGirshick. Clevr: A diagnostic dataset for compositional\nlanguage and elementary visual reasoning. In CVPR, 2017.\n2, 5\n[19] Amita Kamath, Jack Hessel, and Kai-Wei Chang. What\u2019s\u201d\nup\u201d with vision-language models? investigating their strug-\ngle with spatial reasoning. arXiv preprint arXiv:2310.19785,\n2023. 1, 2, 3\n[20] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg. Referitgame: Referring to objects in pho-\ntographs of natural scenes. In EMNLP, 2014. 3\n[21] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-\nand-language transformer without convolution or region su-\npervision. In ICML, 2021. 2\n[22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In ICCV, 2023. 3, 6\n[23] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B\nTenenbaum. Human-level concept learning through proba-\nbilistic program induction. Science, 350(6266):1332\u20131338,\n2015. 3\n[24] Jon M Laurent, Joseph D Janizek, Michael Ruzo,\nMichaela M Hinks, Michael J Hammerling, Siddharth\nNarayanan, Manvitha Ponnapati, Andrew D White, and\nSamuel G Rodriques. Lab-bench: Measuring capabilities\nof language models for biology research. arXiv preprint\narXiv:2407.10362, 2024. 3\n[25] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded\nlanguage-image pre-training. In CVPR, 2022.", "mimetype": "text/plain", "start_char_idx": 2200, "end_char_idx": 4897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5aa2686f-712f-48d0-95fa-a9ccac8876f1": {"__data__": {"id_": "5aa2686f-712f-48d0-95fa-a9ccac8876f1", "embedding": null, "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4bc12972-10b0-4afb-82d6-61ffa14b87b1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "b06e04bc65f080c1caf8e830bf95329d1e471798d0ac37fef0fdfe7f8200b6f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e9fc9c4-6a8f-4ad9-b09d-fd1f6742f464", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "44a2ed4bd36c2c7093cb6b1f0f651fc51f768174f3ff14f44d2545ddcd9e22fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Science, 350(6266):1332\u20131338,\n2015. 3\n[24] Jon M Laurent, Joseph D Janizek, Michael Ruzo,\nMichaela M Hinks, Michael J Hammerling, Siddharth\nNarayanan, Manvitha Ponnapati, Andrew D White, and\nSamuel G Rodriques. Lab-bench: Measuring capabilities\nof language models for biology research. arXiv preprint\narXiv:2407.10362, 2024. 3\n[25] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded\nlanguage-image pre-training. In CVPR, 2022. 2\n[26] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David\nAcuna. Reasoning paths with reference objects elicit quanti-\ntative spatial reasoning in large vision-language models. In\nEMNLP, 2024. 3\n[27] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection. arXiv preprint\narXiv:2303.05499, 2023. 3\n[28] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B\nTenenbaum, and Jiajun Wu. The neuro-symbolic concept\n9", "mimetype": "text/plain", "start_char_idx": 4364, "end_char_idx": 5462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d9ad5c4-fcfe-4d50-aacd-243598ca44e5": {"__data__": {"id_": "4d9ad5c4-fcfe-4d50-aacd-243598ca44e5", "embedding": null, "metadata": {"page_label": "10", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aeec18ab-a4cb-4416-8d57-7e4fb4f506f1", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "0fdfb6998e5c1e39564416879cfb880d085f8159271dd350e6742403bd783e02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f04ca234-b8d6-4e58-b723-0516e6b60137", "node_type": "1", "metadata": {}, "hash": "fd4498d9934505db598a4eb54ab8a1074ae8f925e3e1846e9465b03ab2e058ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "learner: Interpreting scenes, words, and sentences from nat-\nural supervision. ICLR, 2019. 3\n[29] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim\nNeumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh\nMahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran\nShen, et al. Simple open-vocabulary object detection. In\nECCV, 2022. 2\n[30] Theo X Olausson, Alex Gu, Benjamin Lipkin, Cedegao E\nZhang, Armando Solar-Lezama, Joshua B Tenenbaum, and\nRoger Levy. Linc: A neurosymbolic approach for logical\nreasoning by combining language models with first-order\nlogic provers. In EMNLP, 2023. 3\n[31] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia\nSegu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth:\nUniversal monocular metric depth estimation. In CVPR,\n2024. 3, 6\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 2\n[33] Ren \u00b4e Ranftl, Katrin Lasinger, David Hafner, Konrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE TPAMI, 2020. 2\n[34] Jennifer J Sun, Megan Tjandrasuwita, Atharva Sehgal, Ar-\nmando Solar-Lezama, Swarat Chaudhuri, Yisong Yue, and\nOmar Costilla Reyes. Neurosymbolic programming for\nscience. In NeurIPS 2022 Workshop on AI for Science:\nProgress and Promises, 2022. 3\n[35] D \u00b4\u0131dac Sur\u00b4\u0131s, Sachit Menon, and Carl V ondrick. Vipergpt: Vi-\nsual inference via python execution for reasoning. In ICCV,\n2023. 2, 5, 6, 8, 1\n[36] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 1, 2, 5, 6\n[37] Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy\nXin, and Swarat Chaudhuri. An in-context learning agent for\nformal theorem-proving. In CoLM, 2024. 3\n[38] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun\nWoo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,\nShusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-\n1: A fully open, vision-centric exploration of multimodal\nllms. NeurIPS, 2024. 1, 2, 3\n[39] Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles\nSutton, and Swarat Chaudhuri. Houdini: Lifelong learning\nas program synthesis. Advances in neural information pro-\ncessing systems, 31, 2018. 3\n[40] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-\nmar. V oyager: An open-ended embodied agent with large\nlanguage models. TMLR, 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2717, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f04ca234-b8d6-4e58-b723-0516e6b60137": {"__data__": {"id_": "f04ca234-b8d6-4e58-b723-0516e6b60137", "embedding": null, "metadata": {"page_label": "10", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aeec18ab-a4cb-4416-8d57-7e4fb4f506f1", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "0fdfb6998e5c1e39564416879cfb880d085f8159271dd350e6742403bd783e02", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d9ad5c4-fcfe-4d50-aacd-243598ca44e5", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "9f4d797601e74506edd4ea9e79df1e4f8a7e09cf050da5daaca3937521fd5c80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cambrian-\n1: A fully open, vision-centric exploration of multimodal\nllms. NeurIPS, 2024. 1, 2, 3\n[39] Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles\nSutton, and Swarat Chaudhuri. Houdini: Lifelong learning\nas program synthesis. Advances in neural information pro-\ncessing systems, 31, 2018. 3\n[40] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-\nmar. V oyager: An open-ended embodied agent with large\nlanguage models. TMLR, 2024. 3\n[41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. NeurIPS, 2022. 5, 8\n[42] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray,\nAlejandra Molina, Michael Terry, and Carrie J Cai.\nPromptchainer: Chaining large language model prompts\nthrough visual programming, 2022. 8\n[43] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. Ai\nchains: Transparent and controllable human-ai interaction by\nchaining large language model prompts. In Proceedings of\nthe 2022 CHI Conference on Human Factors in Computing\nSystems, New York, NY , USA, 2022. Association for Com-\nputing Machinery. 8\n[44] Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li\nFei-Fei, and Saining Xie. Thinking in Space: How Multi-\nmodal Large Language Models See, Remember and Recall\nSpaces. arXiv preprint arXiv:2412.14171, 2024. 3, 5, 1, 2\n[45] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vi-\nsion language pre-training: Aligning texts with visual con-\ncepts. arXiv preprint arXiv:2111.08276, 2021. 2\n10", "mimetype": "text/plain", "start_char_idx": 2218, "end_char_idx": 3828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f37364b8-ee94-4925-8d84-c90e989584a0": {"__data__": {"id_": "f37364b8-ee94-4925-8d84-c90e989584a0", "embedding": null, "metadata": {"page_label": "1", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ca1f90-8479-4a36-b0e4-fd415af66b67", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "fef343020b0f8f8b7b893491c60453a86290442a5a4a3c6b0f551fa4c99f2baa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb6f35e5-8f7a-48c2-8a2a-bcb62b984705", "node_type": "1", "metadata": {}, "hash": "3393d118fcef1c320b0e76a2496c3542bf797adbbe6926921307394435d58271", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Visual Agentic AI for Spatial Reasoning with a Dynamic API\nSupplementary Material\nMethod CLEVR OMNI3D-BENCH\nVLMs\nGPT4o [1] 1.4 0.6\nClaude3.5-Sonnet [2] 0.2 0.6\nLlama3.2 [9] 0.5 1.6\nGemini1.5-Pro [36] 0.3 1.8\nGemini1.5-Flash [36] 0.3 1.1\nMolmo [8] 0.0 0.0\nSpaceMantis [6, 17] 0.0 0.0\nProgram\nSynthesis\nViperGPT [35] 1.1 0.3\nVisProg [12] 0.9 0.3\nV ADAR (ours) 2.9 1.8\nTable 5. Standard deviation across experimental runs.\nV ADAR\u2019s variation is comparable to VLMs on Omni3D, but\nslightly higher than program synthesis methods on CLEVR, de-\nspite achieving significantly higher accuracy.\nSignature (for 10 Qs) Implementation Program (per Q) Execution (per Q)\n20.5\u00b13.6 37.2\u00b114.4 6.5\u00b11.8 35.7\u00b111.8\nTable 6. Runtime for each Agent in seconds.\nThe Appendix includes the prompts used for all agents,\nadditional qualitative examples of V ADAR on CLEVR,\nOMNI 3D-B ENCH , and GQA, and a supplemental qualita-\ntive analysis with standard deviations to compare the ro-\nbustness of approaches.\nA. Prompts\nPredefined Module Signatures. Fig. 9 and Fig. 10\nshow the docstrings of the predefined modules for\nCLEVR and O MNI 3D-B ENCH respectively, which are\nused to initialize the dynamic API. We note that the two\nprompts are virtually identical, with the exception of the\nget 2D object size method, which we omit from our\nexperiments on CLEVR as the dataset defines size as ei-\nther small or large. In Fig. 11, we provide the Python\nimplementation for all of the predefined modules.\nSignature Agent Prompt. Fig. 12 contains the prompt used\nfor the Signature Agent for both CLEVR and O MNI 3D-\nBENCH . We prompt the LLM to only generate signatures\nfor methods when necessary, as we found this avoids re-\ndundant methods with minor changes to previously defined\nmethods. We impose that the name of new methods start\nwith an underscore, to prevent the common failure case of\nmethods sharing names with variables previously defined.\nImplementation Agent Prompt. Fig. 13 and Fig. 14 con-\ntain the prompts used for the Implementation agent on\nCLEVR and O MNI 3D-B ENCH respectively. The prompts\ncontain Weak ICLexamples, illustrating how to implement a\nmodel signature and use the pre-defined modules correctly\nfor simpler queries. This is in contrast to Strong ICL ex-\namples in VisProg and ViperGPT, which provide complete\nprogram examples for full queries using a predefined API.\nIn our framework, where agents dynamically generate the\nAPI, Strong ICL is not feasible.\nAdditionally, the prompts featurePseudo ICL in the form\nof natural language instructions and tips. Similarly to the\npredefined modules, the prompts differ between CLEVR\nand O MNI 3D-B ENCH as the latter considers metric sizes\nand not a binary small or large as in CLEVR. Conse-\nquently, we found it necessary to include natural language\ndefinitions and instructions for reasoning about 2D and 3D\ndimensions in the Implementation prompt on O MNI 3D-\nBENCH .\nProgram Agent Prompt. In Fig. 15 and Fig. 16 we\nshow the prompts for the Program Agent on CLEVR and\nOMNI 3D-B ENCH respectively. In the prompt for CLEVR,\nwe include a list of all available attributes. In both prompts,\nwe include Pseudo ICL in the form of natural language ex-\namples and instructions. For the OMNI 3D-B ENCH prompt,\nwe additionally include tips and definitions for handling 2D\nand 3D dimensions.\nB. Additional Quantitative Analysis\nExperimental Variability. Tab. 1 in the main paper re-\nports the mean performance of all methods across 3 runs.\nTab.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3470, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb6f35e5-8f7a-48c2-8a2a-bcb62b984705": {"__data__": {"id_": "bb6f35e5-8f7a-48c2-8a2a-bcb62b984705", "embedding": null, "metadata": {"page_label": "1", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ca1f90-8479-4a36-b0e4-fd415af66b67", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "fef343020b0f8f8b7b893491c60453a86290442a5a4a3c6b0f551fa4c99f2baa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f37364b8-ee94-4925-8d84-c90e989584a0", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "d9b8a5ecff937cb7b249ff4f519c35b83158646de7567e022507b9ed05b9d531", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conse-\nquently, we found it necessary to include natural language\ndefinitions and instructions for reasoning about 2D and 3D\ndimensions in the Implementation prompt on O MNI 3D-\nBENCH .\nProgram Agent Prompt. In Fig. 15 and Fig. 16 we\nshow the prompts for the Program Agent on CLEVR and\nOMNI 3D-B ENCH respectively. In the prompt for CLEVR,\nwe include a list of all available attributes. In both prompts,\nwe include Pseudo ICL in the form of natural language ex-\namples and instructions. For the OMNI 3D-B ENCH prompt,\nwe additionally include tips and definitions for handling 2D\nand 3D dimensions.\nB. Additional Quantitative Analysis\nExperimental Variability. Tab. 1 in the main paper re-\nports the mean performance of all methods across 3 runs.\nTab. 5 reports the standard deviation on CLEVR and\nOMNI 3D-B ENCH across the same 3 runs. V ADAR\u2019s varia-\ntion is comparable to the VLMs on O MNI 3D-B ENCH , but\nslightly higher than program synthesis methods on both\nbenchmarks. However, V ADAR significantly outperforms\nViperGPT and VisProg, even when accounting for this vari-\nation.\nRuntime. Tab. 6 reports runtime in seconds for our Agents\non an A100 GPU. Notably, when running our method on\n1000+ questions, the Signature and Implementation Agents\nonly run once , therefore their runtime becomes negligible\nto total inference runtime.\nC. More information on OMNI 3D-B ENCH\nOn images sourced from Omni3D [5] we collect a set of\nchallenging questions with the help of human annotators.\nWe omit using templates for questions, as done by oth-\ners [6, 38, 44], to avoid template overfitting, and instead\ninstruct annotators to directly ask questions in free-form\nnatural language, focusing on the scene, object layout and\n1", "mimetype": "text/plain", "start_char_idx": 2720, "end_char_idx": 4439, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e87e519a-aa20-46e9-b301-2c86036b7aec": {"__data__": {"id_": "e87e519a-aa20-46e9-b301-2c86036b7aec", "embedding": null, "metadata": {"page_label": "2", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "09a90510-e7ba-4560-9f63-74eada0d2133", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "124394932d1ccae05b0a1526f5ada3a7fdefc6690ce274b5d384bf6a6c896655", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53090279-d6ab-4503-86e1-53d826c9ecc5", "node_type": "1", "metadata": {}, "hash": "b63d787f4fb366b92debb2c2ade504bce9526aaecd9c756f3aff4e4e22a8ac7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "VSI-Bench-img\nGemini1.5-Pro 49.5\nV ADAR 50.1\nTable 7. Results on VSI-Bench [44]. V ADAR outperforms\nGemini1.5-Pro on a image-based subset of 75 queries from VSI-\nBench that sources the frame that contains all the information nec-\nessary to respond correctly. Notably, V ADAR achieves a 50.1%\naccuracy on this subset, compared to 40.4% on OMNI 3D-B ENCH ,\nhighlighting the challenging nature of our proposed benchmark.\nobject sizes. We discard questions that are simplistic, e.g.\n\u201cIs there a sofa in the image?\u201d or \u201cIs the sofa behind the ta-\nble?\u201d, and only keep queries which involve complex infer-\nence steps in 2D and 3D. OMNI 3D-B ENCH queries roughly\ntarget the following areas of reasoning: relative size and di-\nmensions with hypotheticals, spatial relationships and depth\nreasoning, relative proportions and alignments, and interac-\ntion with other objects. Queries from OMNI 3D-B ENCH can\nbe browsed in https://glab-caltech.github.io/vadar/omni3d-\nbench.html.\nWe compute answers for questions using the 3D annota-\ntions provided in Omni3D [5]. Since the questions are not\ntemplated and thus don\u2019t follow rule-based instructions, we\ncollect answers manually by sourcing the 3D annotations\nprovided by the dataset for each image. This results in 500\nunique and challenging image-question-answer tuples that\ntest diverse aspects of 3D spatial reasoning. The diversity\nand complexity of O MNI 3D-B ENCH is showcased by the\nexamples in Fig. 1, Fig. 4 and Fig. 7.\nOMNI 3D-B ENCH complements CLEVR when assess-\ning 3D spatial understanding. While CLEVR uses tem-\nplated questions, enabling the creation of a large volume\nof image-question-answer pairs, O MNI 3D-B ENCH focuses\non diverse and complex reasoning tasks in free-form lan-\nguage. Together, CLEVR and O MNI 3D-B ENCH provide a\ncomprehensive test for models\u2019 3D spatial reasoning capa-\nbilities. This is evidenced by the relatively low performance\nof modern state-of-the-art AI models on these benchmarks,\nachieving only 20-40% accuracy.\nD. Comparison to VSI-Bench\nConcurrent to our work is VSI-Bench [44], a video under-\nstanding benchmark that focuses on spatial reasoning. VSI-\nBench targets 3D reasoning, but it differs from O MNI 3D-\nBENCH in three critical ways: First, it focuses on video un-\nderstanding and retrieving the appropriate frame to answer a\ngiven query. Second, while queries in VSI-Bench target 3D\nobject attributes, they query absolute measurements, such as\n\u201cWhat is the height of the chair?\u201d. Monolithic VLMs when\nprompted with such questions resort to object priors. For\nexample, GPT4o says: \u201cA chair tends to be 30-40 inches\ntall\u201d. In contrast, O MNI 3D-B ENCH introduces hypotheti-\ncals that require reasoning over scene attributes, evaluating\ntrue 3D spatial reasoning,e.g., \u201cIf the table is 2 meters wide,\nhow tall is the chair?\u201d. Third, VSI-Bench queries are tem-\nplated, which can lead to biased conclusions due to template\noverfitting.\nWe compare V ADAR on VSI-Bench. To decouple frame\nretrieval from image-based reasoning, we create a variant\nof the benchmark by sourcing a subset of 75 queries with\nthe associated frame that contains the information neces-\nsary to address the query. We call this subset VSI-Bench-\nimg. Tab. 7 reports V ADAR\u2019s performance and compares to\nGemini1.5-Pro, which authors report to be the best VLM on\nthe set. From Tab. 7 we observe that V ADAR performs on\npar with the industry-leading Gemini1.5-pro. Importantly,\nV ADAR\u2019s performance on VSI-Bench-img is 10% higher\nthan on O MNI 3D-B ENCH (40.4 vs 50.1) which highlights\nthe more challenging nature of our benchmark.\nE. Qualitative Examples on CLEVR\nFig.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3629, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53090279-d6ab-4503-86e1-53d826c9ecc5": {"__data__": {"id_": "53090279-d6ab-4503-86e1-53d826c9ecc5", "embedding": null, "metadata": {"page_label": "2", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "09a90510-e7ba-4560-9f63-74eada0d2133", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "124394932d1ccae05b0a1526f5ada3a7fdefc6690ce274b5d384bf6a6c896655", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e87e519a-aa20-46e9-b301-2c86036b7aec", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "2c3a01abe430b66fed77fd466f2634661feb259e955c3bdd0a66a297ef8ad7bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We compare V ADAR on VSI-Bench. To decouple frame\nretrieval from image-based reasoning, we create a variant\nof the benchmark by sourcing a subset of 75 queries with\nthe associated frame that contains the information neces-\nsary to address the query. We call this subset VSI-Bench-\nimg. Tab. 7 reports V ADAR\u2019s performance and compares to\nGemini1.5-Pro, which authors report to be the best VLM on\nthe set. From Tab. 7 we observe that V ADAR performs on\npar with the industry-leading Gemini1.5-pro. Importantly,\nV ADAR\u2019s performance on VSI-Bench-img is 10% higher\nthan on O MNI 3D-B ENCH (40.4 vs 50.1) which highlights\nthe more challenging nature of our benchmark.\nE. Qualitative Examples on CLEVR\nFig. 6 shows additional qualitative examples on CLEVR.\nThe correct example showcases the use of API methods for\nrepeated tasks and accurately determining spatial relations.\nThe incorrect example highlights a failure to use same ob-\nject to exclude the original reference object when the ques-\ntions asks for \u201canother\u201d object.\nF. Qualitative Examples on OMNI 3D-B ENCH\nFig. 7 shows additional qualitative examples on O MNI 3D-\nBENCH . Our method is able to correctly estimate 3D dis-\ntances by scaling depth based on the reference scale given\nin the question. An instance where such scaling is done in-\ncorrectly is shown in the last example.\nG. Qualitative Examples on GQA\nFig. 8 shows qualitative examples on GQA [16]. Our\nmethod is able to identify and locate key objects necessary\nto answer questions. It is extremely explicit, locating the\nnearest person in the top right example using pixel distance\nfrom the tree. Some GQA questions have ambiguous an-\nswers, where the shape of the pot is generically \u201cround\u201d\nand the frame of reference for spatial relations is not en-\ntirely clear (i.e., which man in the last example?).\n2", "mimetype": "text/plain", "start_char_idx": 2928, "end_char_idx": 4754, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c3bdc9f-9926-4699-b65c-dc67eb35ce54": {"__data__": {"id_": "2c3bdc9f-9926-4699-b65c-dc67eb35ce54", "embedding": null, "metadata": {"page_label": "3", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "338c2406-61e6-40c5-b0c7-f2d02ce879c8", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "64a0840f6f127a64bd5cd2c8e19813025356618e47f713eaeef9fd29ffeecdf6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 6. V ADAR program outputs on CLEVR.\nFigure 7. V ADAR program outputs on OMNI 3D-B ENCH .\nFigure 8. V ADAR program outputs on GQA [16].\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 143, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "104c2e46-70ff-4765-ad09-6af12aaa430c": {"__data__": {"id_": "104c2e46-70ff-4765-ad09-6af12aaa430c", "embedding": null, "metadata": {"page_label": "4", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "424c989d-da09-43a6-baad-dce8655faafc", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "facc4a82c781fcccfd237189be9a183494e1f8b4f6da5573d3315564fd452c4b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\\\"\\\"\\\"\nLocates objects in an image. Object prompts should be 1 WORD MAX.\nArgs:\nimage (image): Image to search.\nobject_prompt (string): Description of object to locate. Examples: \"spheres\", \"objects\".\nReturns:\nlist: A list of x,y coordinates for all of the objects located in pixel space.\n\\\"\\\"\\\"\ndef loc(image, object_prompt):\n\\\"\\\"\\\"\nAnswers a question about the attributes of an object specified by an x,y coordinate.\nShould not be used for other kinds of questions.\nArgs:\nimage (image): Image of the scene.\nquestion (string): Question about the objects attribute to answer. Examples: \"What color is this?\", \"What material is this?\"\nx (int): X coordinate of the object in pixel space.\ny (int): Y coordinate of the object in pixel space.\nReturns:\nstring: Answer to the question about the object in the image.\n\\\"\\\"\\\"\ndef vqa(image, question, x, y):\n\\\"\\\"\\\"\nReturns the depth of an object specified by an x,y coordinate.\nArgs:\nimage (image): Image of the scene.\nx (int): X coordinate of the object in pixel space.\ny (int): Y coordinate of the object in pixel space.\nReturns:\nfloat: The depth of the object specified by the coordinates.\n\\\"\\\"\\\"\ndef depth(image, x, y):\n\\\"\\\"\\\"\nChecks if two pairs of coordinates correspond to the same object.\nArgs:\nimage (image): Image of the scene.\nx_1 (int): X coordinate of object 1 in pixel space.\ny_1 (int): Y coordinate of object 1 in pixel space.\nx_2 (int): X coordinate of object 2 in pixel space.\ny_2 (int): Y coordinate of object 2 in pixel space.\nReturns:\nbool: True if object 1 is the same object as object 2, False otherwise.\n\\\"\\\"\\\"\ndef same_object(image, x_1, y_1, x_2, y_2):\nFigure 9. Pre-defined Modules for CLEVR . These modules are used to initialize the dynamic API. As CLEVR defines size to be either\nlarge or small, we omit the get 2D object size method.\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1804, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4824e53-e96d-4bfe-bd82-674fca4c6691": {"__data__": {"id_": "a4824e53-e96d-4bfe-bd82-674fca4c6691", "embedding": null, "metadata": {"page_label": "5", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "34b6567e-5b51-47e4-a4ce-89ad91402199", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "6aba90a5bb61712d983f4cebf0bab2b7a9df1896fe3de6506fe7e1189b77d13e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\\\"\\\"\\\"\nLocates objects in an image. Object prompts should be 1 WORD MAX.\nArgs:\nimage (image): Image to search.\nobject_prompt (string): Description of object to locate.\nReturns:\nlist: A list of x,y coordinates for all of the objects located in pixel space.\n\\\"\\\"\\\"\ndef loc(image, object_prompt):\n\\\"\\\"\\\"\nAnswers a question about the attributes of an object specified by an x,y coordinate.\nShould not be used for other kinds of questions.\nArgs:\nimage (image): Image of the scene.\nquestion (string): Question about the objects attribute to answer. Examples: \"What color is this?\", \"What material is this?\"\nx (int): X coordinate of the object in pixel space.\ny (int): Y coordinate of the object in pixel space.\nReturns:\nstring: Answer to the question about the object in the image.\n\\\"\\\"\\\"\ndef vqa(image, question, x, y):\n\\\"\\\"\\\"\nReturns the depth of an object specified by an x,y coordinate.\nArgs:\nimage (image): Image of the scene.\nx (int): X coordinate of the object in pixel space.\ny (int): Y coordinate of the object in pixel space.\nReturns:\nfloat: The depth of the object specified by the coordinates.\n\\\"\\\"\\\"\ndef depth(image, x, y):\n\\\"\\\"\\\"\nChecks if two pairs of coordinates correspond to the same object.\nArgs:\nimage (image): Image of the scene.\nx_1 (int): X coordinate of object 1 in pixel space.\ny_1 (int): Y coordinate of object 1 in pixel space.\nx_2 (int): X coordinate of object 2 in pixel space.\ny_2 (int): Y coordinate of object 2 in pixel space.\nReturns:\nbool: True if object 1 is the same object as object 2, False otherwise.\n\\\"\\\"\\\"\ndef same_object(image, x_1, y_1, x_2, y_2):\n\\\"\\\"\\\"\nReturns the width and height of the object in 2D pixel space.\nArgs:\nimage (image): Image of the scene.\nx (int): X coordinate of the object in pixel space.\ny (int): Y coordinate of the object in pixel space.\nReturns:\ntuple: (width, height) of the object in 2D pixel space.\n\\\"\\\"\\\"\ndef get_2D_object_size(image, x, y):\nFigure 10. Pre-defined Modules for OMNI 3D-B ENCH . These modules are used to initialize the dynamic API.\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2015, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a372b57c-4f76-4ed3-baea-85fc48b5fc7c": {"__data__": {"id_": "a372b57c-4f76-4ed3-baea-85fc48b5fc7c", "embedding": null, "metadata": {"page_label": "6", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628e0357-e7d6-456e-af9a-e48edd798271", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "42fd8534e9669b37cc085267fdbb83d03d02740eb4b52c151a4b05b68036d006", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "def loc(self, image, object_prompt):\npts = molmo(image, \"point to the \" + object_prompt)\nif len(pts) == 0:\n# No points found\nreturn []\nreturn pts\ndef vqa(image, question, x, y):\nmask = sam_2([x, y], \"foreground\") # get sam2 mask at x,y\nbbox = bbox_from_mask(mask) # bbox around sam2 mask\nboxed_image = overlay_box_on_image(image, bbox) # original image with bbox overlaid\nresult = gpt4o(boxed_image, question)\nreturn result\ndef depth(image, x, y):\ndepth_pred = unidepth(image)[\"depth\"] # Predict depth map over image\ndepth_x_y = depth_pred[y, x]\nreturn depth_x_y\ndef same_object(image, x_1, y_1, x_2, y_2):\nmask_1 = sam_2([x_1, y_1], \"foreground\") # get sam2 mask for point 1\nmask_2 = sam_2([x_2, y_2], \"foreground\") # get sam2 mask for point 2\nobj_1_bbox = bbox_from_mask(mask_1) # bbox around sam2 mask\nobj_2_bbox = bbox_from_mask(mask_2) # bbox around sam2 mask\nreturn iou(obj_1_bbox, obj_2_bbox) > 0.92\ndef get_2D_object_size(image, x, y):\nmask = sam_2([x, y], \"foreground\") # get sam2 mask at x,y\nbbox = bbox_from_mask(mask) # bbox around sam2 mask\nwidth = abs(box[0] - box[2])\nheight = abs(box[1] - box[3])\nreturn width, height\nFigure 11. Python Implementation of Predefined Modules. V ADAR uses Molmo [8] for object detection, SAM2 [22] for segmentation,\nGPT4o [1] for VQA, and UniDepth [31] for depth estimation.\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1322, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f26abf54-c60c-4847-86f9-7c1658dbffc2": {"__data__": {"id_": "f26abf54-c60c-4847-86f9-7c1658dbffc2", "embedding": null, "metadata": {"page_label": "7", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2f7fd52-7267-4f8d-9718-1c265fc8a861", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "bbf16405aeb4c03e020f84c15575de3ab187b86bb783ead9ee8e51ab672dd94b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Propose only new method signatures to add to the existing API.\nAvailable Primitives: image, int, string, list, tuple\nCurrent API:\n{current_api_signatures}\nNext, I will ask you a series of questions that reference an image and are solvable with a python program that uses\nthe API I have provided so far. Please propose new method signatures with associated docstrings to add to the API that\nwould help modularize the programs that answer the questions.\nFor each proposed method, output the docstring inside <docstring></docstring> immediately followed by the method\nsignature for the docstring inside <signature></signature>. Do not propose methods that are already in the API.\nPlease ensure that you ONLY add new methods when necessary. Do not add new methods if you can solve the problem with\ncombinations of the previous methods!\nAdded methods should be simple, building minorly on the methods that already exist.\nImportantly, new methods MUST start with an underscore. As an example, you may define a \"_get_material\" method. Please\nensure you ALWAYS start the name with an underscore.\nAgain, output the docstring inside <docstring></docstring> immediately followed by the method signature for the\ndocstring inside <signature></signature>.\n{questions}\nFigure 12. Signature Agent Prompt used for both CLEVR and O MNI 3D-B ENCH .\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1331, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "495bc55f-a763-48dc-8a6f-5a75183b1ea1": {"__data__": {"id_": "495bc55f-a763-48dc-8a6f-5a75183b1ea1", "embedding": null, "metadata": {"page_label": "8", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c32666e6-17d9-4c09-a669-3580c34f3b3e", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "9914251a1f89be4e3edbe17c06f9235e713d1695bec6aaddf6bdbab62d71545f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22941ba0-2d1d-4b5a-87a1-effcbe7304d3", "node_type": "1", "metadata": {}, "hash": "0aa28dc6e29238d61259458af51c8c40374b83b1dee804cab2aae06a50a3b537", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Implement a method given a docstring and method signature, using the API specification as necessary.\nCurrent API:\n{pre_defined_signatures}\n{generated_signatures}\nHere are some examples of how to implement a method given its docstring and signature:\n<docstring>\n\\\"\\\"\\\"\nLocates objects that are on the left of the reference object.\nArgs:\nimage (IMAGE): Image to search.\nref_x (int): X coordinate of reference object in pixel space.\nref_y (int): Y coordinate of reference object in pixel space.\nReturns:\npoints (list): list of [x, y] coordinates for objects in pixel space matching description to the left.\n\\\"\\\"\\\"\n</docstring>\n<signature>def objects_left(image, ref_x, ref_y):</signature>\n<implementation>\nobjects_left = []\nall_objects = loc(image, object_prompt=\u2019objects\u2019)\nfor object_point in all_objects:\nx, y = object_point\nif same_object(image, ref_x, ref_y, x, y):\ncontinue\nif x < ref_x:\nobjects_left.append(object_point)\nreturn objects_left\n</implementation>\n<docstring>\n\\\"\\\"\\\"\nGets the material of the given object.\nArgs:\nimage (IMAGE): Image that the object is contained in.\nref_x (int): X coordinate of reference object in pixel space.\nref_y (int): Y coordinate of reference object in pixel space.\nReturns:\nstr: Material of the object.\n\\\"\\\"\\\"\n</docstring>\n<signature>def object_material(image, ref_x, ref_y):</signature>\n<implementation>\nmaterial = vqa(image=image, question=\u2019What material is this object?\u2019, x=ref_x, y=ref_y)\nreturn material\n</implementation>\n<docstring>\n\\\"\\\"\\\"\nChecks if an object 1 is in front of object 2.\nArgs:\nimage (IMAGE): Image that the object is contained in.\nx_1 (int): X coordinate of object 1 in pixel space.\ny_1 (int): Y coordinate of object 1 in pixel space.\nx_2 (int): X coordinate of object 2 in pixel space.\ny_2 (int): Y coordinate of object 2 in pixel space.\nReturns:\nbool: True if object 1 is in front of object 2, False otherwise\n\\\"\\\"\\\"\n</docstring>\n<signature>def in_front_of(image, x_1, y_1, x_2, y_2):</signature>\n<implementation>\ndepth_1 = depth(image, x_1, y_1)\ndepth_2 = depth(image, x_2, y_2)\nreturn depth_1 < depth_2\n</implementation>\n<docstring>\n\\\"\\\"\\\"\nChecks if object1 has the same size as object2\nArgs:\nimage (IMAGE): Image that the object is contained in.\nx_1 (int): X coordinate of object 1 in pixel space.\ny_1 (int): Y coordinate of object 1 in pixel space.\nx_2 (int): X coordinate of object 2 in pixel space.\ny_2 (int): Y coordinate of object 2 in pixel space.\nReturns:\nbool: True if object 1 has the same size as object 2, False otherwise\n\\\"\\\"\\\"\n</docstring>\n<signature>def same_size(image, x_1, y_1, x_2, y_2):</signature>\n<implementation>\nobject_1_size = vqa(image=image, question=\u2019What size is this object?\u2019, x=x_1, y=y_1)\nobject_2_size = vqa(image=image, question=\u2019What size is this object?\u2019, x=x_2, y=y_2)\nreturn object_1_size == object_2_size\n</implementation>\nHere are some helpful tips:\n1) When you need to search over objects satisfying a condition, remember to check all the objects that satisfy the condition and don\u2019t just return the first one.\n2) You already have an initialized variable named \"image\" - no need to initialize it yourself!\n3) When searching for objects to compare to a reference object, make sure to remove the reference object from the retrieved objects. You can check if two objects are\nthe same with the same_object method.\nDo not define new methods here, simply solve the problem using the existing methods.\nNow, given the following docstring and signature, implement the method, using the API specification as necessary. Output the implementation inside <implementation></\nimplementation>.\nAgain, Output the implementation inside <implementation></implementation>.\n<docstring>{docstring}</docstring>\n<signature>{signature}</signature>\nFigure 13. Implementation Agent Prompt for CLEVR.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3778, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22941ba0-2d1d-4b5a-87a1-effcbe7304d3": {"__data__": {"id_": "22941ba0-2d1d-4b5a-87a1-effcbe7304d3", "embedding": null, "metadata": {"page_label": "8", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c32666e6-17d9-4c09-a669-3580c34f3b3e", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "9914251a1f89be4e3edbe17c06f9235e713d1695bec6aaddf6bdbab62d71545f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "495bc55f-a763-48dc-8a6f-5a75183b1ea1", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "b1d3378e2b5ea72c4cc7958d19511660c0becf1c98a5faa1c48408ce884458f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2) You already have an initialized variable named \"image\" - no need to initialize it yourself!\n3) When searching for objects to compare to a reference object, make sure to remove the reference object from the retrieved objects. You can check if two objects are\nthe same with the same_object method.\nDo not define new methods here, simply solve the problem using the existing methods.\nNow, given the following docstring and signature, implement the method, using the API specification as necessary. Output the implementation inside <implementation></\nimplementation>.\nAgain, Output the implementation inside <implementation></implementation>.\n<docstring>{docstring}</docstring>\n<signature>{signature}</signature>\nFigure 13. Implementation Agent Prompt for CLEVR. This prompt differs from the prompt used for O MNI 3D-B ENCH as we omit\nexamples illustrating usage of the get 2D object size method. The prompt features Weak ICL examples illustrating correct usage of\nthe pre-defined modules, as well as Pseudo ICL in the form of natural language instructions.\n8", "mimetype": "text/plain", "start_char_idx": 3017, "end_char_idx": 4075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cadb02b5-eb1f-489c-921e-5e10c023b292": {"__data__": {"id_": "cadb02b5-eb1f-489c-921e-5e10c023b292", "embedding": null, "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "20d250c1-698e-4033-9aec-8a5c398eb4e4", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "c7ead0fad39f04bd98086c6070b3da00154495b0a8621d14252ee3a4b7abee84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55b9e3b0-3a9d-4738-bf05-78f29808e78b", "node_type": "1", "metadata": {}, "hash": "81d694e987db35cf3d14ac7f1b66f86be1c4e21aab29c6a83c6daccf0133c400", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Implement a method given a docstring and method signature, using the API specification as necessary.\nCurrent API:\n{predef_signatures}\n{generated_signatures}\nHere are some examples of how to implement a method given its docstring and signature:\n<docstring>\n\\\"\\\"\\\" Locates objects that are on the left of the reference object.\nArgs:\nimage (IMAGE): Image to search.\nref_x (int): X coordinate of reference object in pixel space.\nref_y (int): Y coordinate of reference object in pixel space.\nReturns:\npoints (list): list of [x, y] coordinates for objects in pixel space matching description to the left.\n\\\"\\\"\\\"\n</docstring>\n<signature>def objects_left(image, ref_x, ref_y):</signature><implementation>\nobjects_left = []\nall_objects = loc(image, object_prompt=\u2019objects\u2019)\nfor object_point in all_objects:\nx, y = object_point\nif same_object(image, ref_x, ref_y, x, y):\ncontinue\nif x < ref_x:\nobjects_left.append(object_point)\nreturn objects_left </implementation>\n<docstring>\n\\\"\\\"\\\" Gets the material of the given object.\nArgs:\nimage (IMAGE): Image that the object is contained in.\nref_x (int): X coordinate of reference object in pixel space.\nref_y (int): Y coordinate of reference object in pixel space.\nReturns:\nstr: Material of the object.\n\\\"\\\"\\\"\n</docstring>\n<signature>def object_material(image, ref_x, ref_y):</signature><implementation>\nreturn vqa(image=image, question=\u2019What material is this object?\u2019, x=ref_x, y=ref_y) </implementation>\n<docstring>\n\\\"\\\"\\\" Checks if an object 1 is in front of object 2.\nArgs:\nimage (IMAGE): Image that the object is contained in.\nx_1 (int): X coordinate of object 1 in pixel space.\ny_1 (int): Y coordinate of object 1 in pixel space.\nx_2 (int): X coordinate of object 2 in pixel space.\ny_2 (int): Y coordinate of object 2 in pixel space.\nReturns:\nbool: True if object 1 is in front of object 2, False otherwise\n\\\"\\\"\\\"\n</docstring>\n<signature>def in_front_of(image, x_1, y_1, x_2, y_2):</signature> <implementation>\ndepth_1, depth_2 = depth(image, x_1, y_1), depth(image, x_2, y_2)\nreturn depth_1 < depth_2 </implementation>\n<docstring>\n\\\"\\\"\\\" Checks if object1 has the same size as object2\nArgs:\nimage (IMAGE): Image that the object is contained in.\nx_1 (int): X coordinate of object 1 in pixel space.\ny_1 (int): Y coordinate of object 1 in pixel space.\nx_2 (int): X coordinate of object 2 in pixel space.\ny_2 (int): Y coordinate of object 2 in pixel space.\nepsilon (float): Acceptable margin of error in sizes.\nReturns:\nbool: True if object 1 has the same size as object 2, False otherwise\n\\\"\\\"\\\"\n</docstring>\n<signature>def same_size(image, x_1, y_1, x_2, y_2, epsilon):</signature> <implementation>\nobject_1_height, object_1_width = get_2D_object_size(image, x_1, y_1)\nobject_2_height, object_2_width = get_2D_object_size(image, x_2, y_2)\nreturn abs(object_1_height - object_2_height) < epislon and abs(object_1_width - object_2_width) < epsilon </implementation>\n<docstring>\n\\\"\\\"\\\" Returns a list of objects in the images\nArgs:\nimage (IMAGE): Image to search for objects in\nReturns:\nlist: List of strings corresponding to all of the objects in the image.\n\\\"\\\"\\\"\n</docstring>\n<signature>def get_object_list(image):</signature> <implementation>\nobjects = []\nobject_points = loc(image, object_prompt=\u2019objects\u2019)\nfor object_point in object_coords:\nobj_x, obj_y = object_point\nobjects.append(vqa(image, \"What is this object?\", obj_x, obj_y))\nreturn objects </implementation>\nHere are some helpful definitions:\n1) 2D distance/size refers to distance/size in pixel space. 2) 3D distance/size refers to distance/size in the real world. 3D size is equal to 2D size times the\ndepth of the object.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3624, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55b9e3b0-3a9d-4738-bf05-78f29808e78b": {"__data__": {"id_": "55b9e3b0-3a9d-4738-bf05-78f29808e78b", "embedding": null, "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "20d250c1-698e-4033-9aec-8a5c398eb4e4", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "c7ead0fad39f04bd98086c6070b3da00154495b0a8621d14252ee3a4b7abee84", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cadb02b5-eb1f-489c-921e-5e10c023b292", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "d7e5b824b308fc42e7258ad89c56e72bc8e52d90c250e335df3f9c2ff8c578da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\\\"\\\"\\\"\n</docstring>\n<signature>def get_object_list(image):</signature> <implementation>\nobjects = []\nobject_points = loc(image, object_prompt=\u2019objects\u2019)\nfor object_point in object_coords:\nobj_x, obj_y = object_point\nobjects.append(vqa(image, \"What is this object?\", obj_x, obj_y))\nreturn objects </implementation>\nHere are some helpful definitions:\n1) 2D distance/size refers to distance/size in pixel space. 2) 3D distance/size refers to distance/size in the real world. 3D size is equal to 2D size times the\ndepth of the object. 3) \"On\" is defined as the closest object ABOVE another object. Only use this definition for \"on\". 4) \"Next to\" is defined as the closest object.\n5) Width is the same as length. 6) \"Depth\" measures distance from the camera in 3D.\nHere are some helpful tips:\n1) When you need to search over objects satisfying a condition, remember to check all the objects that satisfy the condition and don\u2019t just return the first one. 2)\nYou already have an initialized variable named \"image\" - no need to initialize it yourself! 3) When searching for objects to compare to a reference object, make sure\nto remove the reference object from the retrieved objects. You can check if two objects are the same with the same_object method. 4) Do not assume that the objects\nyou see in these questions are all of the objects you will see, keep the methods general. 5) If two objects have the same 2D width, then the object with the largest\ndepth has the largest 3D width. 6) If two objects have the same 2D height, then the object with the largest depth has the largest 3D height. 7) 2D sizes convey the\nheight and width in IMAGE SPACE. To convert to height and width in 3D space, it needs to be multiplied by the depth! 8) If you are given a reference size, scale your\noutput predicted size accordingly! Do not define new methods here, simply solve the problem using the existing methods. Now, given the following docstring and\nsignature, implement the method, using the API specification as necessary. Output the implementation inside <implementation></implementation>. Again, Output the\nimplementation inside <implementation></implementation>.\n<docstring>\n{docstring}\n</docstring>\n<signature>{signature}</signature>\nFigure 14. Implementation Agent Prompt for OMNI 3D-B ENCH . The prompt features Weak ICL examples illustrating correct usage of\nthe pre-defined modules, as well as Pseudo ICL in the form of natural language instructions and definitions.\n9", "mimetype": "text/plain", "start_char_idx": 3094, "end_char_idx": 5559, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9430c6a7-4f15-4b81-af99-2bc808b89982": {"__data__": {"id_": "9430c6a7-4f15-4b81-af99-2bc808b89982", "embedding": null, "metadata": {"page_label": "10", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "517d8891-51d9-400b-bcb9-77e275ee3c41", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "aeb0581f32ca1a56daed4fa836196841a57fd18b22bb371c934b36ee0f07832f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You are an expert logician capable of answering spatial reasoning problems with code. You excel at using a predefined\nAPI to break down a difficult question into simpler parts to write a program that answers spatial and complex\nreasoning problem.\nAnswer the following question using a program that utilizes the API to decompose more complicated tasks and solve the\nproblem.\nAvailable sizes are {{small, large}}, available shapes are {{square, sphere, cylinder}}, available material types are\n{{rubber, metal}}, available colors are {{gray, blue, brown, yellow, red, green, purple, cyan}}.\nThe question may feature attributes that are outside of the available ones I specified above. If that\u2019s the case,\nplease replace them to the most appropriate one from the attributes above.\nI am going to give you an example of how you might approach a problem in psuedocode, then I will give you an API and\nsome instructions for you to answer in real code.\nExample:\nQuestion: \"What is the shape of the matte object in front of the red cylinder?\"\nSolution:\n1) Find all the cylinders (loc(image, \u2019cylinders\u2019))\n2) If cylinders are found, loop through each of the cylinders found\n3) For each cylinder found, check if the color of this cylinder is red. Store the red cylinder if you find it and\nbreak from the loop.\n4) Find all the objects.\n5) For each object, check if the object is rubber (matte is not in the available attributes, so we replace it with\nrubber)\n6) For each rubber object O you found, check if the depth of O is less than the depth of the red cylinder\n7) If that is true, return the shape of that object\nNow here is an API of methods, you will want to solve the problem in a logical and sequential manner as I showed you\n------------------ API ------------------\n{pre_defined_signatures}\n{api}\n------------------ API ------------------\nPlease do not use synonyms, even if they are present in the question.\nUsing the provided API, output a program inside the tags <program></program> to answer the question.\nIt is critical that the final answer is stored in a variable called \"final_result\".\nEnsure that the answer is either yes/no, one word, or one number.\nHere are some helpful tips:\n1) When you need to search over objects satisfying a condition, remember to check all the objects that satisfy the\ncondition and don\u2019t just return the first one.\n2) You already have an initialized variable named \"image\" - no need to initialize it yourself! 3) Do not define new\nmethods here, simply solve the problem using the existing methods.\n3) When searching for objects to compare to a reference object, make sure to remove the reference object from the\nretrieved objects. You can check if two objects are the same with the same_object method.\nAgain, available sizes are {{small, large}}, available shapes are {{square, sphere, cylinder}}, available material\ntypes are {{rubber, metal}}, available colors are {{gray, blue, brown, yellow, red, green, purple, cyan}}.\nAgain, answer the question by using the provided API to write a program in the tags <program></program> and ensure the\nprogram stores the answer in a variable called \"final_result\".\nIt is critical that the final answer is stored in a variable called \"final_result\".\nEnsure that the answer is either yes/no, one word, or one number.\nAGAIN, answer the question by using the provided API to write a program in the tags <program></program> and ensure the\nprogram stores the answer in a variable called \"final_result\".\nYou do not need to define a function to answer the question - just write your program in the tags. Assume \"image\" has\nalready been initialized - do not modify it!\n<question>{question}</question>\nFigure 15. Program Agent Prompt for CLEVR. In the prompt, we provide a list of all available attributes in CLEVR, a Pseudo ICL\nexample in natural language, and some helpful tips.\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3847, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce15688b-a998-43d3-a649-594e34f2a9ea": {"__data__": {"id_": "ce15688b-a998-43d3-a649-594e34f2a9ea", "embedding": null, "metadata": {"page_label": "11", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c30296b3-e35b-4f39-b86f-8bb99f1db9d5", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "f74a3ce5bf813d00bddc42b418649ded8009ed3f9dc4edc363caac93b874767e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b89fe6cb-a4d8-4637-99f1-921eee8b71f4", "node_type": "1", "metadata": {}, "hash": "b0f749eeaacba953aa31874eba33dc37d878d09710643a0db0baff5dafcae4fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You are an expert logician capable of answering spatial reasoning problems with code. You excel at using a predefined\nAPI to break down a difficult question into simpler parts to write a program that answers spatial and complex\nreasoning problem.\nAnswer the following question using a program that utilizes the API to decompose more complicated tasks and solve the\nproblem.\nI am going to give you two examples of how you might approach a problem in psuedocode, then I will give you an API and\nsome instructions for you to answer in real code.\nExample 1:\nQuestion: \"What is the shape of the red object in front of the blue pillow?\"\nSolution:\n1) Find all the pillows (loc(image, \u2019pillow\u2019)).\n2) If pillows are found, loop through each of the pillows found.\n3) For each pillow found, check if the color of this pillow is blue. Store the blue pillow if you find it and break\nfrom the loop.\n4) Find all the objects.\n5) For each object, check if the object is red.\n6) For each red object O you found, check if the depth of O is less than the depth of the blue pillow.\n7) If that is true, return the shape of that object.\nExample 2:\nQuestion: \"How many objects have the same color as the metal bowl?\"\nSolution:\n1) Set a counter to 0\n2) Find all the bowls (loc(image, \u2019bowls\u2019)).\n3) If bowls are found, loop through each of the bowls found.\n4) For each bowl found, check if the material of this bowl is metal. Store the metal bowl if you find it and break\nfrom the loop.\n5) Find and store the color of the metal bowl.\n6) Find all the objects.\n7) For each object O, check if O is the same object as the small bowl (same_object(image, metal_bowl_x, metal_bowl_y,\nobject_x, object_y)). If it is, skip it.\n8) For each O you don\u2019t skip, check if the color of O is the same as the color of the metal bowl.\n9) If it is, increment the counter.\n10) When you are done looping, return the counter.\nNow here is an API of methods, you will want to solve the problem in a logical and sequential manner as I showed you\n------------------ API ------------------\n{predef_signatures}\n{api}\n------------------ API ------------------\nPlease do not use synonyms, even if they are present in the question.\nUsing the provided API, output a program inside the tags <program></program> to answer the question.\nIt is critical that the final answer is stored in a variable called \"final_result\".\nEnsure that the answer is either yes/no, one word, or one number.\nHere are some helpful definitions:\n1) 2D distance/size refers to distance/size in pixel space.\n2) 3D distance/size refers to distance/size in the real world. 3D size is equal to 2D size times the depth of the\nobject.\n3) \"On\" is defined as the closest object ABOVE another object. Only use this definition for \"on\".\n4) \"Next to\" is defined as the closest object.\n5) Width is the same as length.\n6) \"Depth\" measures distance from the camera in 3D.\nHere are some helpful tips:\n1) When you need to search over objects satisfying a condition, remember to check all the objects that satisfy the\ncondition and don\u2019t just return the first one.\n2) You already have an initialized variable named \"image\" - no need to initialize it yourself!\n3) When searching for objects to compare to a reference object, make sure to remove the reference object from the\nretrieved objects. You can check if two objects are the same with the same_object method.\n4) Do not assume that the objects you see in these questions rae all of the objects you will see, keep the methods\ngeneral.\n5) If two objects have the same 2D width, then the object with the largest depth has the largest 3D width.\n6) If two objects have the same 2D height, then the object with the largest depth has the largest 3D height.\n7) 2D sizes convey the height and width in IMAGE SPACE. To convert to height and width in 3D space, it needs to be\nmultiplied by the depth!\n8) If you are given a reference size, scale your output predicted size accordingly!\nAgain, answer the question by using the provided API to write a program in the tags <program></program> and ensure the\nprogram stores the answer in a variable called \"final_result\".", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b89fe6cb-a4d8-4637-99f1-921eee8b71f4": {"__data__": {"id_": "b89fe6cb-a4d8-4637-99f1-921eee8b71f4", "embedding": null, "metadata": {"page_label": "11", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c30296b3-e35b-4f39-b86f-8bb99f1db9d5", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "f74a3ce5bf813d00bddc42b418649ded8009ed3f9dc4edc363caac93b874767e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce15688b-a998-43d3-a649-594e34f2a9ea", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}, "hash": "2c3284ddf275655669d48d2835423f3c99463b6189cf7485bbe5cd5f933078cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can check if two objects are the same with the same_object method.\n4) Do not assume that the objects you see in these questions rae all of the objects you will see, keep the methods\ngeneral.\n5) If two objects have the same 2D width, then the object with the largest depth has the largest 3D width.\n6) If two objects have the same 2D height, then the object with the largest depth has the largest 3D height.\n7) 2D sizes convey the height and width in IMAGE SPACE. To convert to height and width in 3D space, it needs to be\nmultiplied by the depth!\n8) If you are given a reference size, scale your output predicted size accordingly!\nAgain, answer the question by using the provided API to write a program in the tags <program></program> and ensure the\nprogram stores the answer in a variable called \"final_result\".\nIt is critical that the final answer is stored in a variable called \"final_result\".\nEnsure that the answer is either yes/no, one word, or one number.\nAGAIN, answer the question by using the provided API to write a program in the tags <program></program> and ensure the\nprogram stores the answer in a variable called \"final_result\".\nYou do not need to define a function to answer the question - just write your program in the tags. Assume \"image\" has\nalready been initialized - do not modify it!\n<question>{question}</question>\nFigure 16. Program Agent Prompt for OMNI 3D-B ENCH . The prompt features Pseudo ICL in the form of two natural language examples\nand helpful tips for handling 2D and 3D dimensions.\n11", "mimetype": "text/plain", "start_char_idx": 3289, "end_char_idx": 4816, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"e0d6e422-0a18-426e-a265-df534c16f15a": {"node_ids": ["d3c7215b-66ac-4da7-b6eb-c74ef1f8d72b"], "metadata": {"page_label": "1", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "a95a2aeb-0ecf-4696-a427-3e341b3df175": {"node_ids": ["82149970-b769-4e85-ba76-5c82042b38c7", "41646838-e44b-4f92-b3f9-2352aee3b270"], "metadata": {"page_label": "2", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "dda03ec5-6147-4ed3-88b8-98cb2a2871aa": {"node_ids": ["cf667c32-3f4f-4795-a7ee-f86577e42b6e", "a8605c42-dee2-4ded-9ffd-a02c49a95f09"], "metadata": {"page_label": "3", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "318c1074-7f40-42ba-a51e-22e18e141ea4": {"node_ids": ["bf575bae-5bde-455a-bc2d-98af1b61d0f8"], "metadata": {"page_label": "4", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "3a975e2b-6fd7-4f9b-9ef5-1c5f0557fde5": {"node_ids": ["90e95fad-8171-4790-8503-14359a2cf289", "f262115b-e579-4fe6-a3f3-b872c05747b6"], "metadata": {"page_label": "5", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "2e78b880-2bf8-4a86-87e0-d102570ee179": {"node_ids": ["e2a8683b-3c7b-4a67-af60-996befea0c43", "775dae96-69c0-4a03-ab6e-5d490ea914ea"], "metadata": {"page_label": "6", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "41e9d02f-7d9e-4dd2-85f7-8cd3cf48e2fc": {"node_ids": ["e51191d0-7aaf-4d55-a687-292c456ac936"], "metadata": {"page_label": "7", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "ce758829-0c38-406c-9d34-99cf3b9cfb7a": {"node_ids": ["a82810d6-3525-4e78-9488-075619653cb5", "6e64cfe7-17fa-4099-9eca-1706ab9a40cf"], "metadata": {"page_label": "8", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "4bc12972-10b0-4afb-82d6-61ffa14b87b1": {"node_ids": ["c2bed3b4-3c8c-407c-a4d6-2183baba9fee", "0e9fc9c4-6a8f-4ad9-b09d-fd1f6742f464", "5aa2686f-712f-48d0-95fa-a9ccac8876f1"], "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "aeec18ab-a4cb-4416-8d57-7e4fb4f506f1": {"node_ids": ["4d9ad5c4-fcfe-4d50-aacd-243598ca44e5", "f04ca234-b8d6-4e58-b723-0516e6b60137"], "metadata": {"page_label": "10", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "45ca1f90-8479-4a36-b0e4-fd415af66b67": {"node_ids": ["f37364b8-ee94-4925-8d84-c90e989584a0", "bb6f35e5-8f7a-48c2-8a2a-bcb62b984705"], "metadata": {"page_label": "1", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "09a90510-e7ba-4560-9f63-74eada0d2133": {"node_ids": ["e87e519a-aa20-46e9-b301-2c86036b7aec", "53090279-d6ab-4503-86e1-53d826c9ecc5"], "metadata": {"page_label": "2", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "338c2406-61e6-40c5-b0c7-f2d02ce879c8": {"node_ids": ["2c3bdc9f-9926-4699-b65c-dc67eb35ce54"], "metadata": {"page_label": "3", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "424c989d-da09-43a6-baad-dce8655faafc": {"node_ids": ["104c2e46-70ff-4765-ad09-6af12aaa430c"], "metadata": {"page_label": "4", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "34b6567e-5b51-47e4-a4ce-89ad91402199": {"node_ids": ["a4824e53-e96d-4bfe-bd82-674fca4c6691"], "metadata": {"page_label": "5", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "628e0357-e7d6-456e-af9a-e48edd798271": {"node_ids": ["a372b57c-4f76-4ed3-baea-85fc48b5fc7c"], "metadata": {"page_label": "6", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "d2f7fd52-7267-4f8d-9718-1c265fc8a861": {"node_ids": ["f26abf54-c60c-4847-86f9-7c1658dbffc2"], "metadata": {"page_label": "7", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "c32666e6-17d9-4c09-a669-3580c34f3b3e": {"node_ids": ["495bc55f-a763-48dc-8a6f-5a75183b1ea1", "22941ba0-2d1d-4b5a-87a1-effcbe7304d3"], "metadata": {"page_label": "8", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "20d250c1-698e-4033-9aec-8a5c398eb4e4": {"node_ids": ["cadb02b5-eb1f-489c-921e-5e10c023b292", "55b9e3b0-3a9d-4738-bf05-78f29808e78b"], "metadata": {"page_label": "9", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "517d8891-51d9-400b-bcb9-77e275ee3c41": {"node_ids": ["9430c6a7-4f15-4b81-af99-2bc808b89982"], "metadata": {"page_label": "10", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}, "c30296b3-e35b-4f39-b86f-8bb99f1db9d5": {"node_ids": ["ce15688b-a998-43d3-a649-594e34f2a9ea", "b89fe6cb-a4d8-4637-99f1-921eee8b71f4"], "metadata": {"page_label": "11", "file_name": "Visual Agentic AI for Spatial Reasoning.pdf", "file_path": "D:\\RJ\\ProjPython\\livekit1\\data\\Visual Agentic AI for Spatial Reasoning.pdf", "file_type": "application/pdf", "file_size": 26272597, "creation_date": "2025-02-11", "last_modified_date": "2025-02-11"}}}}